{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3228d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0.dev20250801+cu129\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a111e3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 5070 Ti\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe8f58c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s\\miniconda3\\envs\\samsungProj\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f141c4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"unsloth/Qwen2.5-Coder-7B-Instruct-bnb-4bit\"\n",
    "from transformers import BitsAndBytesConfig\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    \n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0023913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s\\miniconda3\\envs\\samsungProj\\Lib\\site-packages\\transformers\\quantizers\\auto.py:226: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",         \n",
    "    quantization_config=bnb_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2529f5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer:\n",
      "USER: Write a Python function that implements a graph-based pathfinding algorithm to find the shortest path between two nodes in a weighted, undirected graph, considering both distance and cost constraints. The function should:\n",
      "1. Take a graph (as an adjacency list with weights and costs), start node, end node, and a maximum cost budget as input.\n",
      "2. Return the shortest path (list of nodes) that respects the cost budget, or None if no valid path exists.\n",
      "3. Use Dijkstraâ€™s algorithm with a modified priority queue to account for both distance and cost.\n",
      "4. Include error handling for invalid inputs (e.g., negative weights, non-existent nodes).\n",
      "5. Provide an example usage with a graph containing at least 5 nodes, demonstrating both a successful path and a case where no path fits the budget.\n",
      "\n",
      "Example graph format:\n",
      "graph = {\n",
      "    'A': {'B': (2, 5), 'C': (4, 10)},  # (distance, cost)\n",
      "    'B': {'A': (2, 5), 'D': (3, 8)},\n",
      "    'C': {'A': (4, 10), 'D': (1, 3), 'E': (5, 12)},\n",
      "    'D': {'B': (3, 8), 'C': (1, 3), 'E': (2, 6)},\n",
      "    'E': {'C': (5, 12), 'D': (2, 6)}.\n",
      "    AI:\n",
      "}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter probs (Tensor of shape (1, 316, 1000)) of distribution Categorical(probs: torch.Size([1, 316, 1000])) to satisfy the constraint Simplex(), but found invalid values:\ntensor([[[7.0129e-02, 5.7709e-02, 5.6366e-02,  ..., 8.9645e-05,\n          8.9645e-05, 8.9645e-05],\n         [2.0374e-01, 1.1896e-01, 1.1438e-01,  ..., 1.9670e-05,\n          1.9670e-05, 1.9670e-05],\n         [8.4766e-01, 3.7506e-02, 3.2837e-02,  ..., 6.5565e-07,\n          6.5565e-07, 6.5565e-07],\n         ...,\n         [2.2998e-01, 1.2494e-01, 7.0618e-02,  ..., 3.1710e-05,\n          3.1710e-05, 3.1710e-05],\n         [8.5059e-01, 1.0077e-01, 1.3878e-02,  ..., 6.5565e-07,\n          6.5565e-07, 6.5565e-07],\n         [5.4883e-01, 3.3521e-01, 2.7115e-02,  ..., 6.1393e-06,\n          6.1393e-06, 6.1393e-06]]], device='cuda:0', dtype=torch.float16)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     50\u001b[39m         probs_seq = torch.ones_like(probs_seq) / probs_seq.size(-\u001b[32m1\u001b[39m)\n\u001b[32m     52\u001b[39m     \u001b[38;5;66;03m# Compute entropy\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     entropy_per_token = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdistributions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs_seq\u001b[49m\u001b[43m)\u001b[49m.entropy()\n\u001b[32m     54\u001b[39m     avg_entropy = entropy_per_token.mean().item()\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Compute sequence-level entropy\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# probs_seq = F.softmax(logits, dim=-1)\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# probs_seq = torch.clamp(probs_seq, min=1e-8)  # clamp any 0s\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# probs_seq = probs_seq / probs_seq.sum(dim=-1, keepdim=True)  # Normalize to sum to 1\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# entropy_per_token = torch.distributions.Categorical(probs_seq).entropy()\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# avg_entropy = entropy_per_token.mean().item()\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\s\\miniconda3\\envs\\samsungProj\\Lib\\site-packages\\torch\\distributions\\categorical.py:81\u001b[39m, in \u001b[36mCategorical.__init__\u001b[39m\u001b[34m(self, probs, logits, validate_args)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28mself\u001b[39m._num_events = \u001b[38;5;28mself\u001b[39m._param.size()[-\u001b[32m1\u001b[39m]\n\u001b[32m     78\u001b[39m batch_shape = (\n\u001b[32m     79\u001b[39m     \u001b[38;5;28mself\u001b[39m._param.size()[:-\u001b[32m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._param.ndimension() > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch.Size()\n\u001b[32m     80\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\s\\miniconda3\\envs\\samsungProj\\Lib\\site-packages\\torch\\distributions\\distribution.py:77\u001b[39m, in \u001b[36mDistribution.__init__\u001b[39m\u001b[34m(self, batch_shape, event_shape, validate_args)\u001b[39m\n\u001b[32m     75\u001b[39m         valid = constraint.check(value)\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._is_all_true(valid):\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     78\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     79\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value.shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     80\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     81\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     82\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     83\u001b[39m             )\n\u001b[32m     84\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n",
      "\u001b[31mValueError\u001b[39m: Expected parameter probs (Tensor of shape (1, 316, 1000)) of distribution Categorical(probs: torch.Size([1, 316, 1000])) to satisfy the constraint Simplex(), but found invalid values:\ntensor([[[7.0129e-02, 5.7709e-02, 5.6366e-02,  ..., 8.9645e-05,\n          8.9645e-05, 8.9645e-05],\n         [2.0374e-01, 1.1896e-01, 1.1438e-01,  ..., 1.9670e-05,\n          1.9670e-05, 1.9670e-05],\n         [8.4766e-01, 3.7506e-02, 3.2837e-02,  ..., 6.5565e-07,\n          6.5565e-07, 6.5565e-07],\n         ...,\n         [2.2998e-01, 1.2494e-01, 7.0618e-02,  ..., 3.1710e-05,\n          3.1710e-05, 3.1710e-05],\n         [8.5059e-01, 1.0077e-01, 1.3878e-02,  ..., 6.5565e-07,\n          6.5565e-07, 6.5565e-07],\n         [5.4883e-01, 3.3521e-01, 2.7115e-02,  ..., 6.1393e-06,\n          6.1393e-06, 6.1393e-06]]], device='cuda:0', dtype=torch.float16)"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "# Write a function that reverses a string. with 2 different programming language. \n",
    "\n",
    "prompt = \"\"\"USER: Write a Python function that implements a graph-based pathfinding algorithm to find the shortest path between two nodes in a weighted, undirected graph, considering both distance and cost constraints. The function should:\n",
    "1. Take a graph (as an adjacency list with weights and costs), start node, end node, and a maximum cost budget as input.\n",
    "2. Return the shortest path (list of nodes) that respects the cost budget, or None if no valid path exists.\n",
    "3. Use Dijkstraâ€™s algorithm with a modified priority queue to account for both distance and cost.\n",
    "4. Include error handling for invalid inputs (e.g., negative weights, non-existent nodes).\n",
    "5. Provide an example usage with a graph containing at least 5 nodes, demonstrating both a successful path and a case where no path fits the budget.\n",
    "\n",
    "Example graph format:\n",
    "graph = {\n",
    "    'A': {'B': (2, 5), 'C': (4, 10)},  # (distance, cost)\n",
    "    'B': {'A': (2, 5), 'D': (3, 8)},\n",
    "    'C': {'A': (4, 10), 'D': (1, 3), 'E': (5, 12)},\n",
    "    'D': {'B': (3, 8), 'C': (1, 3), 'E': (2, 6)},\n",
    "    'E': {'C': (5, 12), 'D': (2, 6)}.\n",
    "    AI:\n",
    "}\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=4096)\n",
    "result = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(\"Generated answer:\")\n",
    "print(result)\n",
    "\n",
    "# Get logits for uncertainty estimation\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, return_dict=True)\n",
    "    logits = outputs.logits  # Shape: (1, seq_len, vocab_size)\n",
    "    \n",
    "    # Check for NaNs or Infs in logits\n",
    "    if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
    "        print(\"Warning: NaNs or Infs in logits. Replacing with zeros.\")\n",
    "        logits = torch.where(torch.isnan(logits) | torch.isinf(logits), torch.zeros_like(logits), logits)\n",
    "    \n",
    "    # Use top-k logits to reduce vocabulary size\n",
    "    k = 1000  # Focus on top-1000 tokens\n",
    "    top_k_logits, _ = torch.topk(logits, k, dim=-1)\n",
    "    \n",
    "    # Compute probabilities with log_softmax for stability\n",
    "    log_probs = F.log_softmax(top_k_logits, dim=-1)\n",
    "    probs_seq = torch.exp(log_probs)  # Convert to probabilities\n",
    "    probs_seq = probs_seq + 1e-8  # Add smoothing\n",
    "    probs_seq = probs_seq / probs_seq.sum(dim=-1, keepdim=True)  # Normalize\n",
    "    \n",
    "    # Fallback: If probs_seq is still invalid, use uniform distribution\n",
    "    if torch.isnan(probs_seq).any() or torch.isinf(probs_seq).any() or (probs_seq.sum(dim=-1) < 0.99).any():\n",
    "        print(\"Warning: Invalid probabilities detected. Using uniform distribution.\")\n",
    "        probs_seq = torch.ones_like(probs_seq) / probs_seq.size(-1)\n",
    "    \n",
    "    # Compute entropy\n",
    "    entropy_per_token = torch.distributions.Categorical(probs_seq).entropy()\n",
    "    avg_entropy = entropy_per_token.mean().item()\n",
    "\n",
    "# Compute sequence-level entropy\n",
    "# probs_seq = F.softmax(logits, dim=-1)\n",
    "# probs_seq = torch.clamp(probs_seq, min=1e-8)  # clamp any 0s\n",
    "# probs_seq = probs_seq / probs_seq.sum(dim=-1, keepdim=True)  # Normalize to sum to 1\n",
    "# entropy_per_token = torch.distributions.Categorical(probs_seq).entropy()\n",
    "# avg_entropy = entropy_per_token.mean().item()\n",
    "\n",
    "print(f\"Avg token entropy: {avg_entropy:.4f}\")\n",
    "print(f\"Allocated VRAM: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "print(f\"Reserved VRAM: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cec66a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samsungProj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
