{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a3c0684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SimpleLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified LLM to show exactly how logits are created\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size=10, embedding_dim=8, hidden_dim=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        # This is what creates the logits in real LLMs\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim, \n",
    "            nhead=2, \n",
    "            dim_feedforward=hidden_dim,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # THE MAGIC LAYER: This outputs the logits!\n",
    "        self.output_projection = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "        self.vocab = [\"the\", \"cat\", \"dog\", \"is\", \"on\", \"mat\", \"running\", \"sleeping\", \"big\", \"small\"]\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        # 1. Convert tokens to embeddings\n",
    "        embeddings = self.embedding(input_ids)  # [batch, seq_len, embedding_dim]\n",
    "        \n",
    "        # 2. Process through transformer\n",
    "        hidden_states = self.transformer_layer(embeddings)  # [batch, seq_len, embedding_dim]\n",
    "        \n",
    "        # 3. PROJECT TO VOCABULARY SIZE - THIS CREATES LOGITS!\n",
    "        logits = self.output_projection(hidden_states)  # [batch, seq_len, vocab_size]\n",
    "        \n",
    "        return logits\n",
    "\n",
    "def explain_logit_creation():\n",
    "    \"\"\"\n",
    "    Show step-by-step how logits are actually created\n",
    "    \"\"\"\n",
    "    print(\"🔍 HOW LOGITS ARE BORN\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    model = SimpleLanguageModel()\n",
    "    \n",
    "    # Input: \"the cat is\"\n",
    "    input_tokens = torch.tensor([[0, 1, 3]])  # token IDs\n",
    "    \n",
    "    print(\"Step 1: Input tokens ->\", [model.vocab[i] for i in input_tokens[0]])\n",
    "    \n",
    "    # Get embeddings\n",
    "    embeddings = model.embedding(input_tokens)\n",
    "    print(f\"Step 2: Token embeddings shape: {embeddings.shape}\")\n",
    "    print(f\"Each token becomes a vector of {embeddings.shape[-1]} numbers\")\n",
    "    \n",
    "    # Process through transformer\n",
    "    hidden_states = model.transformer_layer(embeddings)\n",
    "    print(f\"Step 3: After transformer: {hidden_states.shape}\")\n",
    "    print(\"These are rich representations of each token in context\")\n",
    "    \n",
    "    # THE KEY STEP: Project to vocabulary\n",
    "    logits = model.output_projection(hidden_states)\n",
    "    print(f\"Step 4: Final logits shape: {logits.shape}\")\n",
    "    print(\"Each token position now has a score for EVERY possible next word\")\n",
    "    \n",
    "    # Look at logits for predicting the next word after \"the cat is\"\n",
    "    next_word_logits = logits[0, -1, :]  # Last position\n",
    "    \n",
    "    print(f\"\\n🎯 LOGITS FOR NEXT WORD AFTER 'the cat is':\")\n",
    "    for i, (word, logit) in enumerate(zip(model.vocab, next_word_logits)):\n",
    "        print(f\"{word:>10}: {logit.item():6.2f}\")\n",
    "    \n",
    "    return next_word_logits\n",
    "\n",
    "def why_these_numbers():\n",
    "    \"\"\"\n",
    "    Explain WHY logits have the values they do\n",
    "    \"\"\"\n",
    "    print(\"\\n🤔 WHY DO LOGITS LOOK LIKE RANDOM NUMBERS?\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create some example scenarios\n",
    "    scenarios = {\n",
    "        \"Very confident\": torch.tensor([8.5, 0.1, -0.3, -1.2, -2.1]),\n",
    "        \"Uncertain\": torch.tensor([1.1, 1.0, 0.9, 0.8, 0.7]),\n",
    "        \"One clear winner\": torch.tensor([5.0, -3.0, -4.0, -5.0, -6.0]),\n",
    "        \"Two good options\": torch.tensor([3.0, 2.8, -1.0, -2.0, -3.0])\n",
    "    }\n",
    "    \n",
    "    print(\"SCENARIO ANALYSIS:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for scenario_name, logits in scenarios.items():\n",
    "        probs = F.softmax(logits, dim=0)\n",
    "        \n",
    "        print(f\"\\n{scenario_name}:\")\n",
    "        print(f\"  Logits: {logits.tolist()}\")\n",
    "        print(f\"  Probabilities: {[f'{p:.2f}' for p in probs.tolist()]}\")\n",
    "        print(f\"  Top choice: {probs.max().item():.2f} ({probs.max().item()*100:.0f}%)\")\n",
    "\n",
    "def logit_intuition():\n",
    "    \"\"\"\n",
    "    Build intuition about what logits represent\n",
    "    \"\"\"\n",
    "    print(\"\\n💡 BUILDING INTUITION ABOUT LOGITS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    print(\"Think of logits as 'raw votes' from the neural network:\")\n",
    "    print()\n",
    "    print(\"🗳️  VOTING ANALOGY:\")\n",
    "    print(\"   Logit = 5.0  →  'STRONG YES! Pick this word!'\")\n",
    "    print(\"   Logit = 0.0  →  'Meh, maybe this word'\")\n",
    "    print(\"   Logit = -3.0 →  'NO! Don't pick this word'\")\n",
    "    print()\n",
    "    print(\"🎯 The neural network learned these 'voting weights' during training\")\n",
    "    print(\"   by seeing millions of examples of good next words.\")\n",
    "    print()\n",
    "    print(\"📊 Softmax converts these raw votes into proper probabilities\")\n",
    "    print(\"   that sum to 1.0, but the logits are the 'pure opinion'\")\n",
    "\n",
    "def the_linear_layer_secret():\n",
    "    \"\"\"\n",
    "    Show exactly how the final linear layer creates logits\n",
    "    \"\"\"\n",
    "    print(\"\\n🔬 THE SECRET: IT'S JUST MATRIX MULTIPLICATION!\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Simulate the final layer of an LLM\n",
    "    hidden_size = 4  # Simplified\n",
    "    vocab_size = 5\n",
    "    \n",
    "    # This is what the model learned during training\n",
    "    weight_matrix = torch.tensor([\n",
    "        [ 2.1, -0.5,  1.3,  0.8],  # Weights for word \"cat\" \n",
    "        [-1.2,  3.0, -0.2,  1.5],  # Weights for word \"dog\"\n",
    "        [ 0.3,  0.1,  2.5, -1.0],  # Weights for word \"run\"\n",
    "        [-0.8,  1.8, -1.5,  2.2],  # Weights for word \"sleep\"\n",
    "        [ 1.0, -2.0,  0.5,  0.3],  # Weights for word \"big\"\n",
    "    ])\n",
    "    \n",
    "    bias = torch.tensor([0.1, -0.2, 0.3, 0.0, 0.5])\n",
    "    \n",
    "    # Hidden state from transformer (what the model \"knows\" about context)\n",
    "    hidden_state = torch.tensor([1.5, -0.8, 2.0, 0.5])\n",
    "    \n",
    "    print(\"Weight matrix (learned during training):\")\n",
    "    print(weight_matrix)\n",
    "    print(f\"\\nHidden state (context understanding): {hidden_state}\")\n",
    "    \n",
    "    # The actual logit calculation: logits = W @ h + b\n",
    "    logits = torch.matmul(weight_matrix, hidden_state) + bias\n",
    "    \n",
    "    print(f\"\\nLogits = Weight_matrix @ Hidden_state + Bias\")\n",
    "    print(f\"Logits = {logits.tolist()}\")\n",
    "    \n",
    "    vocab = [\"cat\", \"dog\", \"run\", \"sleep\", \"big\"]\n",
    "    probs = F.softmax(logits, dim=0)\n",
    "    \n",
    "    print(f\"\\nFinal predictions:\")\n",
    "    for word, logit, prob in zip(vocab, logits, probs):\n",
    "        print(f\"{word:>6}: logit={logit:5.2f} → probability={prob:.3f}\")\n",
    "\n",
    "# Run all explanations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e212819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 HOW LOGITS ARE BORN\n",
      "========================================\n",
      "Step 1: Input tokens -> ['the', 'cat', 'is']\n",
      "Step 2: Token embeddings shape: torch.Size([1, 3, 8])\n",
      "Each token becomes a vector of 8 numbers\n",
      "Step 3: After transformer: torch.Size([1, 3, 8])\n",
      "These are rich representations of each token in context\n",
      "Step 4: Final logits shape: torch.Size([1, 3, 10])\n",
      "Each token position now has a score for EVERY possible next word\n",
      "\n",
      "🎯 LOGITS FOR NEXT WORD AFTER 'the cat is':\n",
      "       the:   1.28\n",
      "       cat:  -0.33\n",
      "       dog:  -1.32\n",
      "        is:   0.94\n",
      "        on:   0.69\n",
      "       mat:  -0.22\n",
      "   running:  -0.31\n",
      "  sleeping:   0.18\n",
      "       big:  -0.11\n",
      "     small:   0.16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1.2782, -0.3274, -1.3195,  0.9365,  0.6894, -0.2176, -0.3136,  0.1835,\n",
       "        -0.1090,  0.1588], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explain_logit_creation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aeab3194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤔 WHY DO LOGITS LOOK LIKE RANDOM NUMBERS?\n",
      "==================================================\n",
      "SCENARIO ANALYSIS:\n",
      "------------------------------\n",
      "\n",
      "Very confident:\n",
      "  Logits: [8.5, 0.10000000149011612, -0.30000001192092896, -1.2000000476837158, -2.0999999046325684]\n",
      "  Probabilities: ['1.00', '0.00', '0.00', '0.00', '0.00']\n",
      "  Top choice: 1.00 (100%)\n",
      "\n",
      "Uncertain:\n",
      "  Logits: [1.100000023841858, 1.0, 0.8999999761581421, 0.800000011920929, 0.699999988079071]\n",
      "  Probabilities: ['0.24', '0.22', '0.20', '0.18', '0.16']\n",
      "  Top choice: 0.24 (24%)\n",
      "\n",
      "One clear winner:\n",
      "  Logits: [5.0, -3.0, -4.0, -5.0, -6.0]\n",
      "  Probabilities: ['1.00', '0.00', '0.00', '0.00', '0.00']\n",
      "  Top choice: 1.00 (100%)\n",
      "\n",
      "Two good options:\n",
      "  Logits: [3.0, 2.799999952316284, -1.0, -2.0, -3.0]\n",
      "  Probabilities: ['0.54', '0.44', '0.01', '0.00', '0.00']\n",
      "  Top choice: 0.54 (54%)\n"
     ]
    }
   ],
   "source": [
    "why_these_numbers()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7e04227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💡 BUILDING INTUITION ABOUT LOGITS\n",
      "========================================\n",
      "Think of logits as 'raw votes' from the neural network:\n",
      "\n",
      "🗳️  VOTING ANALOGY:\n",
      "   Logit = 5.0  →  'STRONG YES! Pick this word!'\n",
      "   Logit = 0.0  →  'Meh, maybe this word'\n",
      "   Logit = -3.0 →  'NO! Don't pick this word'\n",
      "\n",
      "🎯 The neural network learned these 'voting weights' during training\n",
      "   by seeing millions of examples of good next words.\n",
      "\n",
      "📊 Softmax converts these raw votes into proper probabilities\n",
      "   that sum to 1.0, but the logits are the 'pure opinion'\n"
     ]
    }
   ],
   "source": [
    "logit_intuition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e169c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔬 THE SECRET: IT'S JUST MATRIX MULTIPLICATION!\n",
      "==================================================\n",
      "Weight matrix (learned during training):\n",
      "tensor([[ 2.1000, -0.5000,  1.3000,  0.8000],\n",
      "        [-1.2000,  3.0000, -0.2000,  1.5000],\n",
      "        [ 0.3000,  0.1000,  2.5000, -1.0000],\n",
      "        [-0.8000,  1.8000, -1.5000,  2.2000],\n",
      "        [ 1.0000, -2.0000,  0.5000,  0.3000]])\n",
      "\n",
      "Hidden state (context understanding): tensor([ 1.5000, -0.8000,  2.0000,  0.5000])\n",
      "\n",
      "Logits = Weight_matrix @ Hidden_state + Bias\n",
      "Logits = [6.649999618530273, -4.050000190734863, 5.170000076293945, -4.539999961853027, 4.75]\n",
      "\n",
      "Final predictions:\n",
      "   cat: logit= 6.65 → probability=0.726\n",
      "   dog: logit=-4.05 → probability=0.000\n",
      "   run: logit= 5.17 → probability=0.165\n",
      " sleep: logit=-4.54 → probability=0.000\n",
      "   big: logit= 4.75 → probability=0.109\n"
     ]
    }
   ],
   "source": [
    "the_linear_layer_secret()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "463e7bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 MODEL PARAMETERS (WEIGHTS) - These are FIXED after training\n",
      "======================================================================\n",
      "Final layer weight matrix:\n",
      "Shape: torch.Size([10, 4]) → [vocab_size, hidden_dimensions]\n",
      "\n",
      "Weights for 'the' (id=0): [2.0999999046325684, -0.5, 1.2999999523162842, 0.800000011920929]\n",
      "Weights for 'cat' (id=1): [-1.2000000476837158, 3.0, -0.20000000298023224, 1.5]\n",
      "Weights for 'dog' (id=2): [0.30000001192092896, 0.10000000149011612, 2.5, -1.0]\n",
      "Weights for 'is' (id=3): [-0.800000011920929, 1.7999999523162842, -1.5, 2.200000047683716]\n",
      "Weights for 'running' (id=4): [1.5, 0.800000011920929, 2.0, 1.2000000476837158]\n",
      "Weights for 'sleeping' (id=5): [0.8999999761581421, -0.30000001192092896, 1.7999999523162842, 0.5]\n",
      "Weights for 'big' (id=6): [1.0, 1.5, -0.20000000298023224, 0.800000011920929]\n",
      "Weights for 'small' (id=7): [-0.5, 2.0, 0.30000001192092896, -0.800000011920929]\n",
      "Weights for 'on' (id=8): [0.699999988079071, -1.0, 1.5, 1.7999999523162842]\n",
      "Weights for 'mat' (id=9): [1.7999999523162842, 0.20000000298023224, -0.5, 1.0]\n",
      "\n",
      "Bias terms: [0.10000000149011612, -0.20000000298023224, 0.30000001192092896, 0.0, 0.5, 0.20000000298023224, -0.10000000149011612, 0.4000000059604645, 0.30000001192092896, 0.10000000149011612]\n",
      "\n",
      "💡 These weights were learned during training and DON'T change!\n",
      "\n",
      "🎯 INFERENCE EXAMPLE: Predicting next word after 'the cat'\n",
      "======================================================================\n",
      "Input tokens: ['the', 'cat'] → IDs: [0, 1]\n",
      "Context vector (from transformer): [1.5, -0.800000011920929, 2.0, 0.5]\n",
      "\n",
      "🧮 COMPUTING LOGITS (this happens every inference)\n",
      "============================================================\n",
      "Input context vector: [1.5, -0.800000011920929, 2.0, 0.5]\n",
      "↓\n",
      "For each possible next word, compute: weights • context + bias\n",
      "\n",
      "LOGIT COMPUTATION FOR EACH WORD:\n",
      "----------------------------------------\n",
      "'the' (id=0):\n",
      "  [2.0999999046325684, -0.5, 1.2999999523162842, 0.800000011920929] • [1.5, -0.800000011920929, 2.0, 0.5] + 0.1\n",
      "  = 6.55 + 0.1 = 6.65\n",
      "\n",
      "'cat' (id=1):\n",
      "  [-1.2000000476837158, 3.0, -0.20000000298023224, 1.5] • [1.5, -0.800000011920929, 2.0, 0.5] + -0.2\n",
      "  = -3.85 + -0.2 = -4.05\n",
      "\n",
      "'dog' (id=2):\n",
      "  [0.30000001192092896, 0.10000000149011612, 2.5, -1.0] • [1.5, -0.800000011920929, 2.0, 0.5] + 0.3\n",
      "  = 4.87 + 0.3 = 5.17\n",
      "\n",
      "'is' (id=3):\n",
      "  [-0.800000011920929, 1.7999999523162842, -1.5, 2.200000047683716] • [1.5, -0.800000011920929, 2.0, 0.5] + 0.0\n",
      "  = -4.54 + 0.0 = -4.54\n",
      "\n",
      "'running' (id=4):\n",
      "  [1.5, 0.800000011920929, 2.0, 1.2000000476837158] • [1.5, -0.800000011920929, 2.0, 0.5] + 0.5\n",
      "  = 6.21 + 0.5 = 6.71\n",
      "\n",
      "'sleeping' (id=5):\n",
      "  [0.8999999761581421, -0.30000001192092896, 1.7999999523162842, 0.5] • [1.5, -0.800000011920929, 2.0, 0.5] + 0.2\n",
      "  = 5.44 + 0.2 = 5.64\n",
      "\n",
      "'big' (id=6):\n",
      "  [1.0, 1.5, -0.20000000298023224, 0.800000011920929] • [1.5, -0.800000011920929, 2.0, 0.5] + -0.1\n",
      "  = 0.30 + -0.1 = 0.20\n",
      "\n",
      "'small' (id=7):\n",
      "  [-0.5, 2.0, 0.30000001192092896, -0.800000011920929] • [1.5, -0.800000011920929, 2.0, 0.5] + 0.4\n",
      "  = -2.15 + 0.4 = -1.75\n",
      "\n",
      "'on' (id=8):\n",
      "  [0.699999988079071, -1.0, 1.5, 1.7999999523162842] • [1.5, -0.800000011920929, 2.0, 0.5] + 0.3\n",
      "  = 5.75 + 0.3 = 6.05\n",
      "\n",
      "'mat' (id=9):\n",
      "  [1.7999999523162842, 0.20000000298023224, -0.5, 1.0] • [1.5, -0.800000011920929, 2.0, 0.5] + 0.1\n",
      "  = 2.04 + 0.1 = 2.14\n",
      "\n",
      "🏆 TOP 5 PREDICTIONS:\n",
      "-------------------------\n",
      "1. 'running' (id=4): logit=6.71, prob=0.330\n",
      "2. 'the' (id=0): logit=6.65, prob=0.311\n",
      "3. 'on' (id=8): logit=6.05, prob=0.171\n",
      "4. 'sleeping' (id=5): logit=5.64, prob=0.113\n",
      "5. 'dog' (id=2): logit=5.17, prob=0.071\n",
      "\n",
      "🎯 INFERENCE EXAMPLE: Predicting next word after 'the big'\n",
      "======================================================================\n",
      "Input tokens: ['the', 'big'] → IDs: [0, 6]\n",
      "Context vector (from transformer): [0.20000000298023224, 1.7999999523162842, -0.5, 1.2000000476837158]\n",
      "\n",
      "🧮 COMPUTING LOGITS (this happens every inference)\n",
      "============================================================\n",
      "Input context vector: [0.20000000298023224, 1.7999999523162842, -0.5, 1.2000000476837158]\n",
      "↓\n",
      "For each possible next word, compute: weights • context + bias\n",
      "\n",
      "LOGIT COMPUTATION FOR EACH WORD:\n",
      "----------------------------------------\n",
      "'the' (id=0):\n",
      "  [2.0999999046325684, -0.5, 1.2999999523162842, 0.800000011920929] • [0.20000000298023224, 1.7999999523162842, -0.5, 1.2000000476837158] + 0.1\n",
      "  = -0.17 + 0.1 = -0.07\n",
      "\n",
      "'cat' (id=1):\n",
      "  [-1.2000000476837158, 3.0, -0.20000000298023224, 1.5] • [0.20000000298023224, 1.7999999523162842, -0.5, 1.2000000476837158] + -0.2\n",
      "  = 7.06 + -0.2 = 6.86\n",
      "\n",
      "'dog' (id=2):\n",
      "  [0.30000001192092896, 0.10000000149011612, 2.5, -1.0] • [0.20000000298023224, 1.7999999523162842, -0.5, 1.2000000476837158] + 0.3\n",
      "  = -2.21 + 0.3 = -1.91\n",
      "\n",
      "'is' (id=3):\n",
      "  [-0.800000011920929, 1.7999999523162842, -1.5, 2.200000047683716] • [0.20000000298023224, 1.7999999523162842, -0.5, 1.2000000476837158] + 0.0\n",
      "  = 6.47 + 0.0 = 6.47\n",
      "\n",
      "'running' (id=4):\n",
      "  [1.5, 0.800000011920929, 2.0, 1.2000000476837158] • [0.20000000298023224, 1.7999999523162842, -0.5, 1.2000000476837158] + 0.5\n",
      "  = 2.18 + 0.5 = 2.68\n",
      "\n",
      "'sleeping' (id=5):\n",
      "  [0.8999999761581421, -0.30000001192092896, 1.7999999523162842, 0.5] • [0.20000000298023224, 1.7999999523162842, -0.5, 1.2000000476837158] + 0.2\n",
      "  = -0.66 + 0.2 = -0.46\n",
      "\n",
      "'big' (id=6):\n",
      "  [1.0, 1.5, -0.20000000298023224, 0.800000011920929] • [0.20000000298023224, 1.7999999523162842, -0.5, 1.2000000476837158] + -0.1\n",
      "  = 3.96 + -0.1 = 3.86\n",
      "\n",
      "'small' (id=7):\n",
      "  [-0.5, 2.0, 0.30000001192092896, -0.800000011920929] • [0.20000000298023224, 1.7999999523162842, -0.5, 1.2000000476837158] + 0.4\n",
      "  = 2.39 + 0.4 = 2.79\n",
      "\n",
      "'on' (id=8):\n",
      "  [0.699999988079071, -1.0, 1.5, 1.7999999523162842] • [0.20000000298023224, 1.7999999523162842, -0.5, 1.2000000476837158] + 0.3\n",
      "  = -0.25 + 0.3 = 0.05\n",
      "\n",
      "'mat' (id=9):\n",
      "  [1.7999999523162842, 0.20000000298023224, -0.5, 1.0] • [0.20000000298023224, 1.7999999523162842, -0.5, 1.2000000476837158] + 0.1\n",
      "  = 2.17 + 0.1 = 2.27\n",
      "\n",
      "🏆 TOP 5 PREDICTIONS:\n",
      "-------------------------\n",
      "1. 'cat' (id=1): logit=6.86, prob=0.564\n",
      "2. 'is' (id=3): logit=6.47, prob=0.382\n",
      "3. 'big' (id=6): logit=3.86, prob=0.028\n",
      "4. 'small' (id=7): logit=2.79, prob=0.010\n",
      "5. 'running' (id=4): logit=2.68, prob=0.009\n",
      "\n",
      "🎯 INFERENCE EXAMPLE: Predicting next word after 'the dog is'\n",
      "======================================================================\n",
      "Input tokens: ['the', 'dog', 'is'] → IDs: [0, 2, 3]\n",
      "Context vector (from transformer): [1.0, 0.0, 1.0, 0.0]\n",
      "\n",
      "🧮 COMPUTING LOGITS (this happens every inference)\n",
      "============================================================\n",
      "Input context vector: [1.0, 0.0, 1.0, 0.0]\n",
      "↓\n",
      "For each possible next word, compute: weights • context + bias\n",
      "\n",
      "LOGIT COMPUTATION FOR EACH WORD:\n",
      "----------------------------------------\n",
      "'the' (id=0):\n",
      "  [2.0999999046325684, -0.5, 1.2999999523162842, 0.800000011920929] • [1.0, 0.0, 1.0, 0.0] + 0.1\n",
      "  = 3.40 + 0.1 = 3.50\n",
      "\n",
      "'cat' (id=1):\n",
      "  [-1.2000000476837158, 3.0, -0.20000000298023224, 1.5] • [1.0, 0.0, 1.0, 0.0] + -0.2\n",
      "  = -1.40 + -0.2 = -1.60\n",
      "\n",
      "'dog' (id=2):\n",
      "  [0.30000001192092896, 0.10000000149011612, 2.5, -1.0] • [1.0, 0.0, 1.0, 0.0] + 0.3\n",
      "  = 2.80 + 0.3 = 3.10\n",
      "\n",
      "'is' (id=3):\n",
      "  [-0.800000011920929, 1.7999999523162842, -1.5, 2.200000047683716] • [1.0, 0.0, 1.0, 0.0] + 0.0\n",
      "  = -2.30 + 0.0 = -2.30\n",
      "\n",
      "'running' (id=4):\n",
      "  [1.5, 0.800000011920929, 2.0, 1.2000000476837158] • [1.0, 0.0, 1.0, 0.0] + 0.5\n",
      "  = 3.50 + 0.5 = 4.00\n",
      "\n",
      "'sleeping' (id=5):\n",
      "  [0.8999999761581421, -0.30000001192092896, 1.7999999523162842, 0.5] • [1.0, 0.0, 1.0, 0.0] + 0.2\n",
      "  = 2.70 + 0.2 = 2.90\n",
      "\n",
      "'big' (id=6):\n",
      "  [1.0, 1.5, -0.20000000298023224, 0.800000011920929] • [1.0, 0.0, 1.0, 0.0] + -0.1\n",
      "  = 0.80 + -0.1 = 0.70\n",
      "\n",
      "'small' (id=7):\n",
      "  [-0.5, 2.0, 0.30000001192092896, -0.800000011920929] • [1.0, 0.0, 1.0, 0.0] + 0.4\n",
      "  = -0.20 + 0.4 = 0.20\n",
      "\n",
      "'on' (id=8):\n",
      "  [0.699999988079071, -1.0, 1.5, 1.7999999523162842] • [1.0, 0.0, 1.0, 0.0] + 0.3\n",
      "  = 2.20 + 0.3 = 2.50\n",
      "\n",
      "'mat' (id=9):\n",
      "  [1.7999999523162842, 0.20000000298023224, -0.5, 1.0] • [1.0, 0.0, 1.0, 0.0] + 0.1\n",
      "  = 1.30 + 0.1 = 1.40\n",
      "\n",
      "🏆 TOP 5 PREDICTIONS:\n",
      "-------------------------\n",
      "1. 'running' (id=4): logit=4.00, prob=0.369\n",
      "2. 'the' (id=0): logit=3.50, prob=0.224\n",
      "3. 'dog' (id=2): logit=3.10, prob=0.150\n",
      "4. 'sleeping' (id=5): logit=2.90, prob=0.123\n",
      "5. 'on' (id=8): logit=2.50, prob=0.082\n",
      "\n",
      "================================================================================\n",
      "🔑 KEY DIFFERENCES: WEIGHTS vs LOGITS\n",
      "================================================================================\n",
      "WEIGHTS/PARAMETERS:\n",
      "✓ Fixed numbers learned during training\n",
      "✓ Stored in model files (.bin, .safetensors)\n",
      "✓ Same for every inference\n",
      "✓ Shape: [vocab_size, hidden_size]\n",
      "✓ Example: tensor([2.1, -0.5, 1.3, 0.8]) for word 'cat'\n",
      "\n",
      "LOGITS:\n",
      "✓ Computed fresh for each query\n",
      "✓ Result of: weights @ context_vector + bias\n",
      "✓ Different for every different input\n",
      "✓ Shape: [vocab_size] - one score per word\n",
      "✓ Example: tensor([4.2, -1.8, 3.1, 0.5, 2.3, ...]) for some context\n",
      "\n",
      "🎯 THE RELATIONSHIP:\n",
      "Weights are like a 'recipe' → Logits are the 'dish' you cook\n",
      "Same recipe + different ingredients = different dish\n",
      "Same weights + different context = different logits\n",
      "\n",
      "================================================================================\n",
      "💡 FOR YOUR PROJECT:\n",
      "================================================================================\n",
      "• You DON'T train new weights - you use pre-trained LLM weights\n",
      "• You DO extract logits from each inference\n",
      "• Uncertainty comes from analyzing the logit patterns\n",
      "• Your router learns to interpret logit uncertainty patterns\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ExplainableLanguageModel:\n",
    "    \"\"\"\n",
    "    Shows the difference between weights (parameters) and logits (outputs)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Create a tiny vocabulary for easy tracking\n",
    "        self.vocab = {\n",
    "            0: \"the\", 1: \"cat\", 2: \"dog\", 3: \"is\", 4: \"running\", \n",
    "            5: \"sleeping\", 6: \"big\", 7: \"small\", 8: \"on\", 9: \"mat\"\n",
    "        }\n",
    "        self.word_to_id = {word: id for id, word in self.vocab.items()}\n",
    "        \n",
    "        # THESE ARE THE PARAMETERS/WEIGHTS (fixed after training)\n",
    "        # Shape: [vocab_size, hidden_size] \n",
    "        self.final_layer_weights = torch.tensor([\n",
    "            # Each row = weights for predicting one word\n",
    "            #    [dim0, dim1, dim2, dim3]  ← hidden dimensions\n",
    "            [  2.1, -0.5,  1.3,  0.8],  # weights for predicting \"the\" (id=0)\n",
    "            [ -1.2,  3.0, -0.2,  1.5],  # weights for predicting \"cat\" (id=1) \n",
    "            [  0.3,  0.1,  2.5, -1.0],  # weights for predicting \"dog\" (id=2)\n",
    "            [ -0.8,  1.8, -1.5,  2.2],  # weights for predicting \"is\" (id=3)\n",
    "            [  1.5,  0.8,  2.0,  1.2],  # weights for predicting \"running\" (id=4)\n",
    "            [  0.9, -0.3,  1.8,  0.5],  # weights for predicting \"sleeping\" (id=5)\n",
    "            [  1.0,  1.5, -0.2,  0.8],  # weights for predicting \"big\" (id=6)\n",
    "            [ -0.5,  2.0,  0.3, -0.8],  # weights for predicting \"small\" (id=7)\n",
    "            [  0.7, -1.0,  1.5,  1.8],  # weights for predicting \"on\" (id=8)\n",
    "            [  1.8,  0.2, -0.5,  1.0]   # weights for predicting \"mat\" (id=9)\n",
    "        ])\n",
    "        \n",
    "        self.bias = torch.tensor([0.1, -0.2, 0.3, 0.0, 0.5, 0.2, -0.1, 0.4, 0.3, 0.1])\n",
    "    \n",
    "    def show_parameters(self):\n",
    "        \"\"\"Show the actual model parameters (weights)\"\"\"\n",
    "        print(\"🔧 MODEL PARAMETERS (WEIGHTS) - These are FIXED after training\")\n",
    "        print(\"=\" * 70)\n",
    "        print(\"Final layer weight matrix:\")\n",
    "        print(\"Shape:\", self.final_layer_weights.shape, \"→ [vocab_size, hidden_dimensions]\")\n",
    "        print()\n",
    "        \n",
    "        for word_id, word in self.vocab.items():\n",
    "            weights = self.final_layer_weights[word_id]\n",
    "            print(f\"Weights for '{word}' (id={word_id}): {weights.tolist()}\")\n",
    "        \n",
    "        print(f\"\\nBias terms: {self.bias.tolist()}\")\n",
    "        print(\"\\n💡 These weights were learned during training and DON'T change!\")\n",
    "    \n",
    "    def compute_logits(self, context_vector, show_computation=True):\n",
    "        \"\"\"\n",
    "        Compute logits from context vector - THIS IS WHERE LOGITS COME FROM\n",
    "        \"\"\"\n",
    "        if show_computation:\n",
    "            print(f\"\\n🧮 COMPUTING LOGITS (this happens every inference)\")\n",
    "            print(\"=\" * 60)\n",
    "            print(f\"Input context vector: {context_vector.tolist()}\")\n",
    "            print(\"↓\")\n",
    "            print(\"For each possible next word, compute: weights • context + bias\")\n",
    "            print()\n",
    "        \n",
    "        # LOGITS = WEIGHTS @ CONTEXT + BIAS\n",
    "        logits = torch.matmul(self.final_layer_weights, context_vector) + self.bias\n",
    "        \n",
    "        if show_computation:\n",
    "            print(\"LOGIT COMPUTATION FOR EACH WORD:\")\n",
    "            print(\"-\" * 40)\n",
    "            for word_id, word in self.vocab.items():\n",
    "                weights = self.final_layer_weights[word_id]\n",
    "                dot_product = torch.dot(weights, context_vector).item()\n",
    "                bias_val = self.bias[word_id].item()\n",
    "                final_logit = logits[word_id].item()\n",
    "                \n",
    "                print(f\"'{word}' (id={word_id}):\")\n",
    "                print(f\"  {weights.tolist()} • {context_vector.tolist()} + {bias_val:.1f}\")\n",
    "                print(f\"  = {dot_product:.2f} + {bias_val:.1f} = {final_logit:.2f}\")\n",
    "                print()\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def demonstrate_inference(self, sentence):\n",
    "        \"\"\"Show complete inference process\"\"\"\n",
    "        print(f\"\\n🎯 INFERENCE EXAMPLE: Predicting next word after '{sentence}'\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Convert sentence to token IDs\n",
    "        words = sentence.split()\n",
    "        token_ids = [self.word_to_id[word] for word in words]\n",
    "        print(f\"Input tokens: {words} → IDs: {token_ids}\")\n",
    "        \n",
    "        # Simulate context vector (in real LLM, this comes from transformer layers)\n",
    "        # Different contexts produce different vectors\n",
    "        if \"cat\" in words:\n",
    "            context_vector = torch.tensor([1.5, -0.8, 2.0, 0.5])  # \"animal context\"\n",
    "        elif \"big\" in words or \"small\" in words:\n",
    "            context_vector = torch.tensor([0.2, 1.8, -0.5, 1.2])  # \"size context\" \n",
    "        else:\n",
    "            context_vector = torch.tensor([1.0, 0.0, 1.0, 0.0])   # \"neutral context\"\n",
    "        \n",
    "        print(f\"Context vector (from transformer): {context_vector.tolist()}\")\n",
    "        \n",
    "        # Compute logits\n",
    "        logits = self.compute_logits(context_vector)\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        probs = F.softmax(logits, dim=0)\n",
    "        \n",
    "        # Show top predictions\n",
    "        top_values, top_indices = torch.topk(logits, 5)\n",
    "        \n",
    "        print(\"🏆 TOP 5 PREDICTIONS:\")\n",
    "        print(\"-\" * 25)\n",
    "        for i, (logit_val, word_id) in enumerate(zip(top_values, top_indices)):\n",
    "            word = self.vocab[word_id.item()]\n",
    "            prob = probs[word_id].item()\n",
    "            print(f\"{i+1}. '{word}' (id={word_id.item()}): logit={logit_val:.2f}, prob={prob:.3f}\")\n",
    "\n",
    "def key_differences():\n",
    "    \"\"\"Highlight the key differences\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🔑 KEY DIFFERENCES: WEIGHTS vs LOGITS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"WEIGHTS/PARAMETERS:\")\n",
    "    print(\"✓ Fixed numbers learned during training\")\n",
    "    print(\"✓ Stored in model files (.bin, .safetensors)\")\n",
    "    print(\"✓ Same for every inference\")\n",
    "    print(\"✓ Shape: [vocab_size, hidden_size]\")\n",
    "    print(\"✓ Example: tensor([2.1, -0.5, 1.3, 0.8]) for word 'cat'\")\n",
    "    \n",
    "    print(\"\\nLOGITS:\")\n",
    "    print(\"✓ Computed fresh for each query\")  \n",
    "    print(\"✓ Result of: weights @ context_vector + bias\")\n",
    "    print(\"✓ Different for every different input\")\n",
    "    print(\"✓ Shape: [vocab_size] - one score per word\")\n",
    "    print(\"✓ Example: tensor([4.2, -1.8, 3.1, 0.5, 2.3, ...]) for some context\")\n",
    "    \n",
    "    print(\"\\n🎯 THE RELATIONSHIP:\")\n",
    "    print(\"Weights are like a 'recipe' → Logits are the 'dish' you cook\")\n",
    "    print(\"Same recipe + different ingredients = different dish\")\n",
    "    print(\"Same weights + different context = different logits\")\n",
    "\n",
    "# Run the demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    model = ExplainableLanguageModel()\n",
    "    \n",
    "    # Show the fixed parameters\n",
    "    model.show_parameters()\n",
    "    \n",
    "    # Show different inferences with same weights but different contexts\n",
    "    model.demonstrate_inference(\"the cat\")\n",
    "    model.demonstrate_inference(\"the big\")\n",
    "    model.demonstrate_inference(\"the dog is\")\n",
    "    \n",
    "    # Explain the key differences\n",
    "    key_differences()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"💡 FOR YOUR PROJECT:\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"• You DON'T train new weights - you use pre-trained LLM weights\")\n",
    "    print(\"• You DO extract logits from each inference\")  \n",
    "    print(\"• Uncertainty comes from analyzing the logit patterns\")\n",
    "    print(\"• Your router learns to interpret logit uncertainty patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e61b0f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samsungProj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
