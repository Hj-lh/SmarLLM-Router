{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d364520e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s\\miniconda3\\envs\\samsungProj\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e35e9b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"unsloth/Qwen2.5-Coder-7B-Instruct-bnb-4bit\"\n",
    "from transformers import BitsAndBytesConfig\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    \n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6858cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s\\miniconda3\\envs\\samsungProj\\Lib\\site-packages\\transformers\\quantizers\\auto.py:226: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",         \n",
    "    quantization_config=bnb_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a176a18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer:\n",
      "USER: Write a Python function that implements a graph-based pathfinding algorithm to find the shortest path between two nodes in a weighted, undirected graph, considering both distance and cost constraints. The function should:\n",
      "1. Take a graph (as an adjacency list with weights and costs), start node, end node, and a maximum cost budget as input.\n",
      "2. Return the shortest path (list of nodes) that respects the cost budget, or None if no valid path exists.\n",
      "3. Use Dijkstra’s algorithm with a modified...\n",
      "\n",
      "==================================================\n",
      "UNCERTAINTY ANALYSIS\n",
      "==================================================\n",
      "Logits shape: torch.Size([1, 316, 152064])\n",
      "Vocab size: 152064\n",
      "\n",
      "ENTROPY:\n",
      "  Mean: 0.6689\n",
      "  Std:  0.8497\n",
      "  Range: [0.0002, 5.0954]\n",
      "\n",
      "MAX_PROB_UNCERTAINTY:\n",
      "  Mean: 0.2066\n",
      "  Std:  0.2541\n",
      "\n",
      "TOP_K_CONCENTRATION:\n",
      "  Mean: 0.0395\n",
      "  Std:  0.0927\n",
      "\n",
      "PROBABILITY_VARIANCE:\n",
      "  Mean: 0.0007\n",
      "  Std:  0.0003\n",
      "\n",
      "==================================================\n",
      "MEMORY USAGE\n",
      "==================================================\n",
      "Allocated VRAM: 8.44 GB\n",
      "Reserved VRAM: 10.46 GB\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"USER: Write a Python function that implements a graph-based pathfinding algorithm to find the shortest path between two nodes in a weighted, undirected graph, considering both distance and cost constraints. The function should:\n",
    "1. Take a graph (as an adjacency list with weights and costs), start node, end node, and a maximum cost budget as input.\n",
    "2. Return the shortest path (list of nodes) that respects the cost budget, or None if no valid path exists.\n",
    "3. Use Dijkstra’s algorithm with a modified priority queue to account for both distance and cost.\n",
    "4. Include error handling for invalid inputs (e.g., negative weights, non-existent nodes).\n",
    "5. Provide an example usage with a graph containing at least 5 nodes, demonstrating both a successful path and a case where no path fits the budget.\n",
    "\n",
    "Example graph format:\n",
    "graph = {\n",
    "    'A': {'B': (2, 5), 'C': (4, 10)},  \n",
    "    'B': {'A': (2, 5), 'D': (3, 8)},\n",
    "    'C': {'A': (4, 10), 'D': (1, 3), 'E': (5, 12)},\n",
    "    'D': {'B': (3, 8), 'C': (1, 3), 'E': (2, 6)},\n",
    "    'E': {'C': (5, 12), 'D': (2, 6)}.\n",
    "    \n",
    "}\n",
    "AI response::\n",
    "\"\"\"\n",
    "messages = [\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "]\n",
    "    \n",
    "    \n",
    "formatted_prompt = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True  \n",
    ")\n",
    "    \n",
    "print(\"Formatted prompt:\")\n",
    "print(repr(formatted_prompt[:200] + \"...\" + formatted_prompt[-100:]))\n",
    "print(\"\\n\")\n",
    "    \n",
    "    \n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "\n",
    "generated_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=2048,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.05,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "new_tokens = generated_ids[0][inputs['input_ids'].shape[1]:]\n",
    "response = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "\n",
    "print(\"GENERATED RESPONSE:\")\n",
    "print(\"=\"*50)\n",
    "print(response)\n",
    "print(\"=\"*50)\n",
    "print(f\"Response length: {len(response)} characters\")\n",
    "\n",
    "def calculate_uncertainty_metrics(logits, method='entropy', top_k=1000):\n",
    "    \"\"\"\n",
    "    Calculate various uncertainty metrics from model logits\n",
    "    \n",
    "    Args:\n",
    "        logits: Raw logits from model (batch_size, seq_len, vocab_size)\n",
    "        method: 'entropy', 'max_prob', 'top_k_ratio', or 'all'\n",
    "        top_k: Number of top tokens to consider (reduces computation)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of uncertainty metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
    "        print(\"Warning: Found NaN/Inf in logits, replacing with large negative values\")\n",
    "        logits = torch.where(torch.isnan(logits) | torch.isinf(logits), \n",
    "                           torch.full_like(logits, -1e6), logits)\n",
    "    \n",
    "    \n",
    "    if top_k < logits.size(-1):\n",
    "        top_k_logits, top_k_indices = torch.topk(logits, top_k, dim=-1)\n",
    "        working_logits = top_k_logits\n",
    "    else:\n",
    "        working_logits = logits\n",
    "    \n",
    "    \n",
    "    working_logits = working_logits.float()\n",
    "    \n",
    "    \n",
    "    log_probs = F.log_softmax(working_logits, dim=-1)\n",
    "    probs = torch.exp(log_probs)\n",
    "    \n",
    "    \n",
    "    epsilon = 1e-8\n",
    "    probs = probs + epsilon\n",
    "    probs = probs / probs.sum(dim=-1, keepdim=True)\n",
    "    \n",
    "    \n",
    "    prob_sums = probs.sum(dim=-1)\n",
    "    if not torch.allclose(prob_sums, torch.ones_like(prob_sums), atol=1e-6):\n",
    "        print(f\"Warning: Probability sums range from {prob_sums.min():.6f} to {prob_sums.max():.6f}\")\n",
    "        \n",
    "        probs = probs / prob_sums.unsqueeze(-1)\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    if method in ['entropy', 'all']:\n",
    "        \n",
    "        entropy_per_token = -(probs * torch.log(probs + epsilon)).sum(dim=-1)\n",
    "        metrics['entropy'] = {\n",
    "            'per_token': entropy_per_token,\n",
    "            'mean': entropy_per_token.mean().item(),\n",
    "            'std': entropy_per_token.std().item(),\n",
    "            'max': entropy_per_token.max().item(),\n",
    "            'min': entropy_per_token.min().item()\n",
    "        }\n",
    "    \n",
    "    if method in ['max_prob', 'all']:\n",
    "        \n",
    "        max_probs, _ = torch.max(probs, dim=-1)\n",
    "        uncertainty_from_max = 1.0 - max_probs  \n",
    "        metrics['max_prob_uncertainty'] = {\n",
    "            'per_token': uncertainty_from_max,\n",
    "            'mean': uncertainty_from_max.mean().item(),\n",
    "            'std': uncertainty_from_max.std().item()\n",
    "        }\n",
    "    \n",
    "    if method in ['top_k_ratio', 'all']:\n",
    "        \n",
    "        k_for_concentration = min(5, probs.size(-1))\n",
    "        top_k_probs, _ = torch.topk(probs, k_for_concentration, dim=-1)\n",
    "        concentration = top_k_probs.sum(dim=-1)\n",
    "        uncertainty_from_concentration = 1.0 - concentration\n",
    "        metrics['top_k_concentration'] = {\n",
    "            'per_token': uncertainty_from_concentration,\n",
    "            'mean': uncertainty_from_concentration.mean().item(),\n",
    "            'std': uncertainty_from_concentration.std().item()\n",
    "        }\n",
    "    \n",
    "    if method in ['variance', 'all']:\n",
    "        \n",
    "        prob_variance = torch.var(probs, dim=-1)\n",
    "        metrics['probability_variance'] = {\n",
    "            'per_token': prob_variance,\n",
    "            'mean': prob_variance.mean().item(),\n",
    "            'std': prob_variance.std().item()\n",
    "        }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"UNCERTAINTY ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    outputs = model(**inputs, return_dict=True)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    print(f\"Logits shape: {logits.shape}\")\n",
    "    print(f\"Vocab size: {logits.shape[-1]}\")\n",
    "    \n",
    "    \n",
    "    uncertainty_metrics = calculate_uncertainty_metrics(logits, method='all', top_k=1000)\n",
    "    \n",
    "    for metric_name, metric_data in uncertainty_metrics.items():\n",
    "        print(f\"\\n{metric_name.upper()}:\")\n",
    "        print(f\"  Mean: {metric_data['mean']:.4f}\")\n",
    "        if 'std' in metric_data:\n",
    "            print(f\"  Std:  {metric_data['std']:.4f}\")\n",
    "        if 'max' in metric_data and 'min' in metric_data:\n",
    "            print(f\"  Range: [{metric_data['min']:.4f}, {metric_data['max']:.4f}]\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"MEMORY USAGE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Allocated VRAM: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "print(f\"Reserved VRAM: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2547ec7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s\\miniconda3\\envs\\samsungProj\\Lib\\site-packages\\transformers\\quantizers\\auto.py:226: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's default chat template:\n",
      "'{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n'\n",
      "\n",
      "============================================================\n",
      "\n",
      "METHOD 1: Using tokenizer.apply_chat_template()\n",
      "Formatted prompt:\n",
      "\"<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nWrite a Python function that implements a graph-based pathfinding algorithm to find t...': (1, 3), 'E': (2, 6)},\\n    'E': {'C': (5, 12), 'D': (2, 6)}\\n}\\n```<|im_end|>\\n<|im_start|>assistant\\n\"\n",
      "\n",
      "\n",
      "GENERATED RESPONSE:\n",
      "==================================================\n",
      "```python\n",
      "import heapq\n",
      "\n",
      "def dijkstra_pathfind(graph, start, end, max_cost):\n",
      "    # Check for invalid inputs\n",
      "    if start not in graph or end not in graph:\n",
      "        raise ValueError(\"Start or end node does not exist in the graph.\")\n",
      "    \n",
      "    # Initialize distances and costs, and previous node pointers\n",
      "    distances = {node: float('inf') for node in graph}\n",
      "    costs = {node: float('inf') for node in graph}\n",
      "    previous_nodes = {node: None for node in graph}\n",
      "    distances[start] = 0\n",
      "    costs[start] = 0\n",
      "    \n",
      "    # Priority queue based on combined distance and cost\n",
      "    priority_queue = [(0, 0, start)]\n",
      "    \n",
      "    while priority_queue:\n",
      "        current_cost, current_distance, current_node = heapq.heappop(priority_queue)\n",
      "        \n",
      "        # If we've reached the end, return the path\n",
      "        if current_node == end:\n",
      "            path = []\n",
      "            while current_node is not None:\n",
      "                path.append(current_node)\n",
      "                current_node = previous_nodes[current_node]\n",
      "            path.reverse()\n",
      "            return path\n",
      "        \n",
      "        # Explore each neighbor\n",
      "        for neighbor, (distance, cost) in graph[current_node].items():\n",
      "            new_distance = current_distance + distance\n",
      "            new_cost = current_cost + cost\n",
      "            \n",
      "            # Only consider paths that respect the cost budget\n",
      "            if new_cost <= max_cost:\n",
      "                if new_distance < distances[neighbor]:\n",
      "                    distances[neighbor] = new_distance\n",
      "                    costs[neighbor] = new_cost\n",
      "                    previous_nodes[neighbor] = current_node\n",
      "                    heapq.heappush(priority_queue, (new_cost, new_distance, neighbor))\n",
      "    \n",
      "    # If no path is found within the budget, return None\n",
      "    return None\n",
      "\n",
      "# Example usage\n",
      "graph = {\n",
      "    'A': {'B': (2, 5), 'C': (4, 10)},\n",
      "    'B': {'A': (2, 5), 'D': (3, 8)},\n",
      "    'C': {'A': (4, 10), 'D': (1, 3), 'E': (5, 12)},\n",
      "    'D': {'B': (3, 8), 'C': (1, 3), 'E': (2, 6)},\n",
      "    'E': {'C': (5, 12), 'D': (2, 6)}\n",
      "}\n",
      "\n",
      "print(dijkstra_pathfind(graph, 'A', 'E', 20))  # Output: ['A', 'C', 'D', 'E']\n",
      "print(dijkstra_pathfind(graph, 'A', 'E', 15))  # Output: None\n",
      "```\n",
      "\n",
      "This solution uses Dijkstra's algorithm to find the shortest path that respects a cost budget in a weighted, undirected graph. It modifies the priority queue to prioritize paths based on both distance and cost, ensuring that the path returned is the most efficient within the given budget constraints.\n",
      "==================================================\n",
      "Response length: 2425 characters\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "METHOD 2: Manual template formatting\n",
      "Manual prompt format:\n",
      "\"<|im_start|>user\\nWrite a Python function that implements a graph-based pathfinding algorithm to find the shortest path between two nodes in a weighted, undirected graph, considering both distance and ...D': (2, 6)}\\n}\\n```<|im_end|>\\n<|im_start|>assistant\\n\"\n",
      "\n",
      "\n",
      "MANUAL TEMPLATE RESPONSE:\n",
      "==================================================\n",
      "```python\n",
      "import heapq\n",
      "\n",
      "def dijkstra_pathfinding(graph, start, end, max_cost):\n",
      "    if start not in graph or end not in graph:\n",
      "        raise ValueError(\"Start or end node does not exist in the graph.\")\n",
      "    \n",
      "    distances = {node: float('inf') for node in graph}\n",
      "    costs = {node: float('inf') for node in graph}\n",
      "    distances[start] = 0\n",
      "    costs[start] = 0\n",
      "    priority_queue = [(0, 0, start)]  # (total_cost, total_distance, current_node)\n",
      "    \n",
      "    while priority_queue:\n",
      "        current_total_cost, current_distance, current_node = heapq.heappop(priority_queue)\n",
      "        \n",
      "        if current_total_cost > max_cost or current_distance > distances[current_node]:\n",
      "            continue\n",
      "        \n",
      "        if current_node == end:\n",
      "            return [node for node, _ in sorted(zip(distances.keys(), distances.values()), key=lambda x: x[1])]\n",
      "        \n",
      "        for neighbor, (distance, cost) in graph[current_node].items():\n",
      "            new_distance = current_distance + distance\n",
      "            new_cost = current_total_cost + cost\n",
      "            \n",
      "            if new_distance < distances[neighbor] or (new_distance == distances[neighbor] and new_cost < costs[neighbor]):\n",
      "                distances[neighbor] = new_distance\n",
      "                costs[neighbor] = new_cost\n",
      "                heapq.heappush(priority_queue, (new_cost, new_distance, neighbor))\n",
      "    \n",
      "    return None\n",
      "\n",
      "# Example usage\n",
      "graph = {\n",
      "    'A': {'B': (2, 5), 'C': (4, 10)},\n",
      "    'B': {'A': (2, 5), 'D': (3, 8)},\n",
      "    'C': {'A': (4, 10), 'D': (1, 3), 'E': (5, 12)},\n",
      "    'D': {'B': (3, 8), 'C': (1, 3), 'E': (2, 6)},\n",
      "    'E': {'C': (5, 12), 'D': (2, 6)}\n",
      "}\n",
      "\n",
      "start_node = 'A'\n",
      "end_node = 'E'\n",
      "max_cost = 20\n",
      "\n",
      "path = dijkstra_pathfinding(graph, start_node, end_node, max_cost)\n",
      "print(\"Shortest path:\", path)\n",
      "```\n",
      "\n",
      "This Python function uses Dijkstra's algorithm to find the shortest path in a weighted, undirected graph that does not exceed a given cost budget. It handles errors for invalid inputs and demonstrates the use of a priority queue to efficiently explore the graph.\n",
      "==================================================\n",
      "Response length: 2006 characters\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "METHOD 3: Debugging tokenizer info\n",
      "Special tokens: {'eos_token': '<|im_end|>', 'pad_token': '<|PAD_TOKEN|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}\n",
      "BOS token: None (ID: None)\n",
      "EOS token: '<|im_end|>' (ID: 151645)\n",
      "UNK token: None (ID: None)\n",
      "PAD token: '<|PAD_TOKEN|>' (ID: 151665)\n",
      "\n",
      "Test chat template result:\n",
      "'<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nHello<|im_end|>\\n<|im_start|>assistant\\n'\n",
      "\n",
      "============================================================\n",
      "UNCERTAINTY ANALYSIS\n",
      "============================================================\n",
      "Logits shape: torch.Size([1, 343, 152064])\n",
      "Average entropy: 0.6594\n",
      "Max entropy: 3.9801\n",
      "Min entropy: 0.0002\n",
      "Entropy trend (first 10): [3.980069160461426, 3.707944393157959, 3.6666760444641113, 0.7219915986061096, 1.632411003112793, 0.00020864765974693, 0.021105946972966194, 0.40343570709228516, 0.10450693219900131, 0.1932453215122223]\n",
      "Entropy trend (last 10): [0.0017613900126889348, 0.31918230652809143, 0.20378336310386658, 0.36541831493377686, 0.07203119248151779, 0.0008620737353339791, 0.00018750075832940638, 0.810375452041626, 0.07139723002910614, 0.11050229519605637]\n",
      "\n",
      "Memory usage:\n",
      "Allocated VRAM: 11.60 GB\n",
      "Reserved VRAM: 13.70 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Your existing setup...\n",
    "model_name = \"unsloth/Qwen2.5-Coder-7B-Instruct-bnb-4bit\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "# Check what chat template the model expects\n",
    "print(\"Model's default chat template:\")\n",
    "print(repr(tokenizer.chat_template))\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# CORRECT WAY: Use proper chat template format\n",
    "user_message = \"\"\"Write a Python function that implements a graph-based pathfinding algorithm to find the shortest path between two nodes in a weighted, undirected graph, considering both distance and cost constraints. The function should:\n",
    "\n",
    "1. Take a graph (as an adjacency list with weights and costs), start node, end node, and a maximum cost budget as input.\n",
    "2. Return the shortest path (list of nodes) that respects the cost budget, or None if no valid path exists.\n",
    "3. Use Dijkstra's algorithm with a modified priority queue to account for both distance and cost.\n",
    "4. Include error handling for invalid inputs (e.g., negative weights, non-existent nodes).\n",
    "5. Provide an example usage with a graph containing at least 5 nodes, demonstrating both a successful path and a case where no path fits the budget.\n",
    "\n",
    "Example graph format:\n",
    "```python\n",
    "graph = {\n",
    "    'A': {'B': (2, 5), 'C': (4, 10)},  # (distance, cost)\n",
    "    'B': {'A': (2, 5), 'D': (3, 8)},\n",
    "    'C': {'A': (4, 10), 'D': (1, 3), 'E': (5, 12)},\n",
    "    'D': {'B': (3, 8), 'C': (1, 3), 'E': (2, 6)},\n",
    "    'E': {'C': (5, 12), 'D': (2, 6)}\n",
    "}\n",
    "```\"\"\"\n",
    "\n",
    "# Method 1: Use the built-in chat template (RECOMMENDED)\n",
    "print(\"METHOD 1: Using tokenizer.apply_chat_template()\")\n",
    "try:\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template and tokenize\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True  # This adds the assistant's turn\n",
    "    )\n",
    "    \n",
    "    print(\"Formatted prompt:\")\n",
    "    print(repr(formatted_prompt[:200] + \"...\" + formatted_prompt[-100:]))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Tokenize the formatted prompt\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate with proper parameters\n",
    "    generated_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=2048,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.05,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    # Decode only the new tokens (exclude the input)\n",
    "    new_tokens = generated_ids[0][inputs['input_ids'].shape[1]:]\n",
    "    response = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    print(\"GENERATED RESPONSE:\")\n",
    "    print(\"=\"*50)\n",
    "    print(response)\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Response length: {len(response)} characters\")\n",
    "    \n",
    "    # Store this for uncertainty analysis\n",
    "    successful_inputs = inputs\n",
    "    successful_generated = generated_ids\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Method 1 failed: {e}\")\n",
    "    print(f\"Error type: {type(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "\n",
    "# Method 2: Manual template formatting (fallback)\n",
    "print(\"METHOD 2: Manual template formatting\")\n",
    "try:\n",
    "    # Qwen2.5 typically uses this format\n",
    "    manual_prompt = f\"<|im_start|>user\\n{user_message}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    \n",
    "    print(\"Manual prompt format:\")\n",
    "    print(repr(manual_prompt[:200] + \"...\" + manual_prompt[-50:]))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    inputs = tokenizer(manual_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    generated_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=2048,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.05,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    # Decode only the new tokens\n",
    "    new_tokens = generated_ids[0][inputs['input_ids'].shape[1]:]\n",
    "    response = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    print(\"MANUAL TEMPLATE RESPONSE:\")\n",
    "    print(\"=\"*50)\n",
    "    print(response)\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Response length: {len(response)} characters\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Method 2 failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "\n",
    "# Method 3: Check tokenizer special tokens\n",
    "print(\"METHOD 3: Debugging tokenizer info\")\n",
    "print(f\"Special tokens: {tokenizer.special_tokens_map}\")\n",
    "print(f\"BOS token: {repr(tokenizer.bos_token)} (ID: {tokenizer.bos_token_id})\")\n",
    "print(f\"EOS token: {repr(tokenizer.eos_token)} (ID: {tokenizer.eos_token_id})\")\n",
    "print(f\"UNK token: {repr(tokenizer.unk_token)} (ID: {tokenizer.unk_token_id})\")\n",
    "print(f\"PAD token: {repr(tokenizer.pad_token)} (ID: {tokenizer.pad_token_id})\")\n",
    "\n",
    "# Check what tokens are being added\n",
    "test_messages = [{\"role\": \"user\", \"content\": \"Hello\"}]\n",
    "test_formatted = tokenizer.apply_chat_template(test_messages, tokenize=False, add_generation_prompt=True)\n",
    "print(f\"\\nTest chat template result:\")\n",
    "print(repr(test_formatted))\n",
    "\n",
    "# Now do uncertainty analysis on the successful generation\n",
    "if 'successful_inputs' in locals() and 'successful_generated' in locals():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"UNCERTAINTY ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get logits for the successful generation\n",
    "        outputs = model(**successful_inputs, return_dict=True)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        print(f\"Logits shape: {logits.shape}\")\n",
    "        \n",
    "        # Calculate entropy using the corrected method from before\n",
    "        def calculate_entropy_safe(logits, top_k=1000):\n",
    "            # Use top-k to reduce computation\n",
    "            if top_k < logits.size(-1):\n",
    "                top_k_logits, _ = torch.topk(logits, top_k, dim=-1)\n",
    "                working_logits = top_k_logits.float()\n",
    "            else:\n",
    "                working_logits = logits.float()\n",
    "            \n",
    "            # Calculate probabilities safely\n",
    "            log_probs = F.log_softmax(working_logits, dim=-1)\n",
    "            probs = torch.exp(log_probs)\n",
    "            \n",
    "            # Add epsilon and normalize\n",
    "            epsilon = 1e-8\n",
    "            probs = probs + epsilon\n",
    "            probs = probs / probs.sum(dim=-1, keepdim=True)\n",
    "            \n",
    "            # Manual entropy calculation\n",
    "            entropy_per_token = -(probs * torch.log(probs + epsilon)).sum(dim=-1)\n",
    "            return entropy_per_token\n",
    "        \n",
    "        entropy_per_token = calculate_entropy_safe(logits)\n",
    "        avg_entropy = entropy_per_token.mean().item()\n",
    "        max_entropy = entropy_per_token.max().item()\n",
    "        min_entropy = entropy_per_token.min().item()\n",
    "        \n",
    "        print(f\"Average entropy: {avg_entropy:.4f}\")\n",
    "        print(f\"Max entropy: {max_entropy:.4f}\")\n",
    "        print(f\"Min entropy: {min_entropy:.4f}\")\n",
    "        \n",
    "        # Show entropy pattern over the sequence\n",
    "        if entropy_per_token.numel() > 1:\n",
    "            print(f\"Entropy trend (first 10): {entropy_per_token[0, :10].tolist()}\")\n",
    "            print(f\"Entropy trend (last 10): {entropy_per_token[0, -10:].tolist()}\")\n",
    "\n",
    "print(f\"\\nMemory usage:\")\n",
    "print(f\"Allocated VRAM: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "print(f\"Reserved VRAM: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0674a539",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s\\miniconda3\\envs\\samsungProj\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\s\\miniconda3\\envs\\samsungProj\\Lib\\site-packages\\transformers\\quantizers\\auto.py:226: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's default chat template:\n",
      "'{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n'\n",
      "\n",
      "============================================================\n",
      "\n",
      "METHOD 1: Using tokenizer.apply_chat_template()\n",
      "Formatted prompt:\n",
      "\"<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nWrite a Python function that implements a graph-based pathfinding algorithm to find t...': (1, 3), 'E': (2, 6)},\\n    'E': {'C': (5, 12), 'D': (2, 6)}\\n}\\n```<|im_end|>\\n<|im_start|>assistant\\n\"\n",
      "\n",
      "\n",
      "GENERATED RESPONSE:\n",
      "==================================================\n",
      "To solve this problem, we can use a modified version of Dijkstra's algorithm that considers both distance and cost constraints. We'll use a priority queue to explore the shortest paths first, but we'll also keep track of the cumulative cost to ensure it doesn't exceed the given budget. Here's how you can implement this in Python:\n",
      "\n",
      "```python\n",
      "import heapq\n",
      "\n",
      "def dijkstra_with_cost(graph, start, end, max_cost):\n",
      "    if start not in graph or end not in graph:\n",
      "        raise ValueError(\"Start or end node does not exist in the graph.\")\n",
      "    \n",
      "    # Priority queue: (distance, total_cost, path)\n",
      "    pq = [(0, 0, [start])]\n",
      "    visited = set()\n",
      "    \n",
      "    while pq:\n",
      "        current_distance, total_cost, path = heapq.heappop(pq)\n",
      "        \n",
      "        # If we reach the end node, return the path\n",
      "        if path[-1] == end:\n",
      "            return path\n",
      "        \n",
      "        # If the current node is already visited, skip it\n",
      "        if path[-1] in visited:\n",
      "            continue\n",
      "        \n",
      "        # Mark the current node as visited\n",
      "        visited.add(path[-1])\n",
      "        \n",
      "        # Explore neighbors\n",
      "        for neighbor, (distance, cost) in graph[path[-1]].items():\n",
      "            if cost + total_cost > max_cost:\n",
      "                continue\n",
      "            \n",
      "            new_path = path + [neighbor]\n",
      "            heapq.heappush(pq, (current_distance + distance, total_cost + cost, new_path))\n",
      "    \n",
      "    # If no valid path is found, return None\n",
      "    return None\n",
      "\n",
      "# Example usage\n",
      "graph = {\n",
      "    'A': {'B': (2, 5), 'C': (4, 10)},  # (distance, cost)\n",
      "    'B': {'A': (2, 5), 'D': (3, 8)},\n",
      "    'C': {'A': (4, 10), 'D': (1, 3), 'E': (5, 12)},\n",
      "    'D': {'B': (3, 8), 'C': (1, 3), 'E': (2, 6)},\n",
      "    'E': {'C': (5, 12), 'D': (2, 6)}\n",
      "}\n",
      "\n",
      "# Test case where a valid path exists within the cost budget\n",
      "path = dijkstra_with_cost(graph, 'A', 'E', 20)\n",
      "print(\"Path from A to E with max cost 20:\", path)\n",
      "\n",
      "# Test case where no valid path exists within the cost budget\n",
      "path = dijkstra_with_cost(graph, 'A', 'E', 10)\n",
      "print(\"Path from A to E with max cost 10:\", path)\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "1. **Priority Queue**: We use a priority queue to always expand the path with the smallest distance first. Each entry in the queue is a tuple containing the cumulative distance, cumulative cost, and the current path.\n",
      "\n",
      "2. **Visited Set**: We maintain a set of visited nodes to avoid processing the same node multiple times, which helps in optimizing the search.\n",
      "\n",
      "3. **Cost Constraint**: Before adding a neighbor to the queue, we check if the additional cost would exceed the `max_cost`. If it does, we skip that neighbor.\n",
      "\n",
      "4. **Path Construction**: As we explore each node, we construct the path by appending the current node to the previous path.\n",
      "\n",
      "5. **Error Handling**: The function raises a `ValueError` if either the start or end node is not present in the graph.\n",
      "\n",
      "This implementation efficiently finds the shortest path while respecting the cost constraint, using a combination of Dijkstra's algorithm and a priority queue.\n",
      "==================================================\n",
      "Response length: 2949 characters\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "METHOD 2: Manual template formatting\n",
      "Manual prompt format:\n",
      "\"<|im_start|>user\\nWrite a Python function that implements a graph-based pathfinding algorithm to find the shortest path between two nodes in a weighted, undirected graph, considering both distance and ...D': (2, 6)}\\n}\\n```<|im_end|>\\n<|im_start|>assistant\\n\"\n",
      "\n",
      "\n",
      "MANUAL TEMPLATE RESPONSE:\n",
      "==================================================\n",
      "To solve this problem, we can use a modified version of Dijkstra's algorithm that takes into account both distance and cost constraints. We'll use a priority queue to efficiently explore the shortest paths while keeping track of the total cost. Here's a Python implementation of such an algorithm:\n",
      "\n",
      "```python\n",
      "import heapq\n",
      "\n",
      "def dijkstra_with_cost(graph, start, end, max_cost):\n",
      "    # Priority queue: (total_distance, total_cost, path)\n",
      "    pq = [(0, 0, [start])]\n",
      "    \n",
      "    # Dictionary to store the best known distance and cost to each node\n",
      "    best_costs = {node: float('inf') for node in graph}\n",
      "    best_costs[start] = 0\n",
      "    \n",
      "    while pq:\n",
      "        total_distance, total_cost, path = heapq.heappop(pq)\n",
      "        \n",
      "        # If we reach the end node within the cost budget, return the path\n",
      "        if path[-1] == end and total_cost <= max_cost:\n",
      "            return path\n",
      "        \n",
      "        # Explore neighbors\n",
      "        for neighbor, (distance, cost) in graph.get(path[-1], {}).items():\n",
      "            new_distance = total_distance + distance\n",
      "            new_cost = total_cost + cost\n",
      "            \n",
      "            # Check if this path is better than any previously found to this node\n",
      "            if new_cost < best_costs[neighbor]:\n",
      "                best_costs[neighbor] = new_cost\n",
      "                new_path = path + [neighbor]\n",
      "                # Only add to the queue if the new cost is within the budget\n",
      "                if new_cost <= max_cost:\n",
      "                    heapq.heappush(pq, (new_distance, new_cost, new_path))\n",
      "    \n",
      "    # If no path is found within the cost budget, return None\n",
      "    return None\n",
      "\n",
      "# Example usage\n",
      "graph = {\n",
      "    'A': {'B': (2, 5), 'C': (4, 10)},  # (distance, cost)\n",
      "    'B': {'A': (2, 5), 'D': (3, 8)},\n",
      "    'C': {'A': (4, 10), 'D': (1, 3), 'E': (5, 12)},\n",
      "    'D': {'B': (3, 8), 'C': (1, 3), 'E': (2, 6)},\n",
      "    'E': {'C': (5, 12), 'D': (2, 6)}\n",
      "}\n",
      "\n",
      "# Successful path within cost budget\n",
      "path = dijkstra_with_cost(graph, 'A', 'E', 20)\n",
      "print(\"Path from A to E within cost budget:\", path)\n",
      "\n",
      "# Path that does not fit within the cost budget\n",
      "path = dijkstra_with_cost(graph, 'A', 'E', 10)\n",
      "print(\"Path from A to E within cost budget:\", path)\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "1. **Priority Queue**: We use a priority queue to always expand the least costly path first. Each entry in the queue is a tuple `(total_distance, total_cost, path)`, where `total_distance` is the cumulative distance from the start node, `total_cost` is the cumulative cost, and `path` is the list of nodes visited so far.\n",
      "\n",
      "2. **Best Costs Dictionary**: This dictionary keeps track of the minimum cost encountered to reach each node. This helps in avoiding redundant work by only considering paths that offer a lower cost to a node.\n",
      "\n",
      "3. **Path Exploration**: For each node, we explore its neighbors and calculate the new total distance and cost. If the new cost is less than the previously recorded cost for that node, we update the cost and push the new path onto the priority queue.\n",
      "\n",
      "4. **Cost Constraint**: We only add a new path to the priority queue if its total cost is within the specified budget.\n",
      "\n",
      "5. **Return Paths**: The function returns the shortest path that fits within the cost budget, or `None` if no such path exists.\n",
      "\n",
      "This implementation efficiently finds the shortest path considering both distance and cost constraints using a modified Dijkstra's algorithm.\n",
      "==================================================\n",
      "Response length: 3326 characters\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "METHOD 3: Debugging tokenizer info\n",
      "Special tokens: {'eos_token': '<|im_end|>', 'pad_token': '<|PAD_TOKEN|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}\n",
      "BOS token: None (ID: None)\n",
      "EOS token: '<|im_end|>' (ID: 151645)\n",
      "UNK token: None (ID: None)\n",
      "PAD token: '<|PAD_TOKEN|>' (ID: 151665)\n",
      "\n",
      "Test chat template result:\n",
      "'<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nHello<|im_end|>\\n<|im_start|>assistant\\n'\n",
      "\n",
      "============================================================\n",
      "UNCERTAINTY ANALYSIS\n",
      "============================================================\n",
      "Logits shape: torch.Size([1, 343, 152064])\n",
      "Average entropy: 0.5631\n",
      "Max entropy: 5.4987\n",
      "Min entropy: 0.0002\n",
      "Entropy trend (first 10): [5.4987335205078125, 5.335977554321289, 0.5100303292274475, 0.06577730923891068, 1.4205737113952637, 0.0015480598667636514, 0.09225104004144669, 0.9045683145523071, 0.11563480645418167, 0.019802605733275414]\n",
      "Entropy trend (last 10): [0.0004519694484770298, 0.16758611798286438, 0.005570663139224052, 0.09993103891611099, 0.06948443502187729, 0.0002009723975788802, 0.0001871187414508313, 0.06347498297691345, 1.0640642642974854, 0.983181357383728]\n",
      "\n",
      "Memory usage:\n",
      "Allocated VRAM: 9.43 GB\n",
      "Reserved VRAM: 9.72 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Your existing setup...\n",
    "model_name = \"unsloth/Qwen2.5-Coder-14B-Instruct-bnb-4bit\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "# Check what chat template the model expects\n",
    "print(\"Model's default chat template:\")\n",
    "print(repr(tokenizer.chat_template))\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# CORRECT WAY: Use proper chat template format\n",
    "user_message = \"\"\"Write a Python function that implements a graph-based pathfinding algorithm to find the shortest path between two nodes in a weighted, undirected graph, considering both distance and cost constraints. The function should:\n",
    "\n",
    "1. Take a graph (as an adjacency list with weights and costs), start node, end node, and a maximum cost budget as input.\n",
    "2. Return the shortest path (list of nodes) that respects the cost budget, or None if no valid path exists.\n",
    "3. Use Dijkstra's algorithm with a modified priority queue to account for both distance and cost.\n",
    "4. Include error handling for invalid inputs (e.g., negative weights, non-existent nodes).\n",
    "5. Provide an example usage with a graph containing at least 5 nodes, demonstrating both a successful path and a case where no path fits the budget.\n",
    "\n",
    "Example graph format:\n",
    "```python\n",
    "graph = {\n",
    "    'A': {'B': (2, 5), 'C': (4, 10)},  # (distance, cost)\n",
    "    'B': {'A': (2, 5), 'D': (3, 8)},\n",
    "    'C': {'A': (4, 10), 'D': (1, 3), 'E': (5, 12)},\n",
    "    'D': {'B': (3, 8), 'C': (1, 3), 'E': (2, 6)},\n",
    "    'E': {'C': (5, 12), 'D': (2, 6)}\n",
    "}\n",
    "```\"\"\"\n",
    "\n",
    "# Method 1: Use the built-in chat template (RECOMMENDED)\n",
    "print(\"METHOD 1: Using tokenizer.apply_chat_template()\")\n",
    "try:\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template and tokenize\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True  # This adds the assistant's turn\n",
    "    )\n",
    "    \n",
    "    print(\"Formatted prompt:\")\n",
    "    print(repr(formatted_prompt[:200] + \"...\" + formatted_prompt[-100:]))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Tokenize the formatted prompt\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate with proper parameters\n",
    "    generated_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=2048,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.05,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    # Decode only the new tokens (exclude the input)\n",
    "    new_tokens = generated_ids[0][inputs['input_ids'].shape[1]:]\n",
    "    response = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    print(\"GENERATED RESPONSE:\")\n",
    "    print(\"=\"*50)\n",
    "    print(response)\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Response length: {len(response)} characters\")\n",
    "    \n",
    "    # Store this for uncertainty analysis\n",
    "    successful_inputs = inputs\n",
    "    successful_generated = generated_ids\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Method 1 failed: {e}\")\n",
    "    print(f\"Error type: {type(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "\n",
    "# Method 2: Manual template formatting (fallback)\n",
    "print(\"METHOD 2: Manual template formatting\")\n",
    "try:\n",
    "    # Qwen2.5 typically uses this format\n",
    "    manual_prompt = f\"<|im_start|>user\\n{user_message}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    \n",
    "    print(\"Manual prompt format:\")\n",
    "    print(repr(manual_prompt[:200] + \"...\" + manual_prompt[-50:]))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    inputs = tokenizer(manual_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    generated_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=2048,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.05,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    # Decode only the new tokens\n",
    "    new_tokens = generated_ids[0][inputs['input_ids'].shape[1]:]\n",
    "    response = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    print(\"MANUAL TEMPLATE RESPONSE:\")\n",
    "    print(\"=\"*50)\n",
    "    print(response)\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Response length: {len(response)} characters\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Method 2 failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "\n",
    "# Method 3: Check tokenizer special tokens\n",
    "print(\"METHOD 3: Debugging tokenizer info\")\n",
    "print(f\"Special tokens: {tokenizer.special_tokens_map}\")\n",
    "print(f\"BOS token: {repr(tokenizer.bos_token)} (ID: {tokenizer.bos_token_id})\")\n",
    "print(f\"EOS token: {repr(tokenizer.eos_token)} (ID: {tokenizer.eos_token_id})\")\n",
    "print(f\"UNK token: {repr(tokenizer.unk_token)} (ID: {tokenizer.unk_token_id})\")\n",
    "print(f\"PAD token: {repr(tokenizer.pad_token)} (ID: {tokenizer.pad_token_id})\")\n",
    "\n",
    "# Check what tokens are being added\n",
    "test_messages = [{\"role\": \"user\", \"content\": \"Hello\"}]\n",
    "test_formatted = tokenizer.apply_chat_template(test_messages, tokenize=False, add_generation_prompt=True)\n",
    "print(f\"\\nTest chat template result:\")\n",
    "print(repr(test_formatted))\n",
    "\n",
    "# Now do uncertainty analysis on the successful generation\n",
    "if 'successful_inputs' in locals() and 'successful_generated' in locals():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"UNCERTAINTY ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get logits for the successful generation\n",
    "        outputs = model(**successful_inputs, return_dict=True)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        print(f\"Logits shape: {logits.shape}\")\n",
    "        \n",
    "        # Calculate entropy using the corrected method from before\n",
    "        def calculate_entropy_safe(logits, top_k=1000):\n",
    "            # Use top-k to reduce computation\n",
    "            if top_k < logits.size(-1):\n",
    "                top_k_logits, _ = torch.topk(logits, top_k, dim=-1)\n",
    "                working_logits = top_k_logits.float()\n",
    "            else:\n",
    "                working_logits = logits.float()\n",
    "            \n",
    "            # Calculate probabilities safely\n",
    "            log_probs = F.log_softmax(working_logits, dim=-1)\n",
    "            probs = torch.exp(log_probs)\n",
    "            \n",
    "            # Add epsilon and normalize\n",
    "            epsilon = 1e-8\n",
    "            probs = probs + epsilon\n",
    "            probs = probs / probs.sum(dim=-1, keepdim=True)\n",
    "            \n",
    "            # Manual entropy calculation\n",
    "            entropy_per_token = -(probs * torch.log(probs + epsilon)).sum(dim=-1)\n",
    "            return entropy_per_token\n",
    "        \n",
    "        entropy_per_token = calculate_entropy_safe(logits)\n",
    "        avg_entropy = entropy_per_token.mean().item()\n",
    "        max_entropy = entropy_per_token.max().item()\n",
    "        min_entropy = entropy_per_token.min().item()\n",
    "        \n",
    "        print(f\"Average entropy: {avg_entropy:.4f}\")\n",
    "        print(f\"Max entropy: {max_entropy:.4f}\")\n",
    "        print(f\"Min entropy: {min_entropy:.4f}\")\n",
    "        \n",
    "        # Show entropy pattern over the sequence\n",
    "        if entropy_per_token.numel() > 1:\n",
    "            print(f\"Entropy trend (first 10): {entropy_per_token[0, :10].tolist()}\")\n",
    "            print(f\"Entropy trend (last 10): {entropy_per_token[0, -10:].tolist()}\")\n",
    "\n",
    "print(f\"\\nMemory usage:\")\n",
    "print(f\"Allocated VRAM: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "print(f\"Reserved VRAM: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388c28ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samsungProj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
