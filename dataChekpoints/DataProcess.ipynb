{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1d9889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7609553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy_metrics(output):\n",
    "    \"\"\"Calculate entropy-based uncertainty metrics\"\"\"\n",
    "    if not output.scores:\n",
    "        return {'mean_entropy': 0.0, 'max_entropy': 0.0, 'min_entropy': 0.0, 'std_entropy': 0.0}\n",
    "    \n",
    "    entropies = []\n",
    "    for score in output.scores:\n",
    "        logits = score[0]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        entropy = -(probs * torch.log(probs + 1e-8)).sum().item()\n",
    "        entropies.append(entropy)\n",
    "    \n",
    "    return {\n",
    "        'mean_entropy': np.mean(entropies),\n",
    "        'max_entropy': np.max(entropies),\n",
    "        'min_entropy': np.min(entropies),\n",
    "        'std_entropy': np.std(entropies)\n",
    "    }\n",
    "\n",
    "def calculate_confidence_metrics(output):\n",
    "    \"\"\"Calculate confidence-based metrics (max probability per token)\"\"\"\n",
    "    if not output.scores:\n",
    "        return {'mean_confidence': 0.0, 'min_confidence': 0.0, 'std_confidence': 0.0}\n",
    "    \n",
    "    confidences = []\n",
    "    for score in output.scores:\n",
    "        logits = score[0]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        max_prob = torch.max(probs).item()\n",
    "        confidences.append(max_prob)\n",
    "    \n",
    "    return {\n",
    "        'mean_confidence': np.mean(confidences),\n",
    "        'min_confidence': np.min(confidences),\n",
    "        'std_confidence': np.std(confidences)\n",
    "    }\n",
    "\n",
    "def calculate_concentration_metrics(output):\n",
    "    \"\"\"Calculate top-k concentration metrics\"\"\"\n",
    "    if not output.scores:\n",
    "        return {'mean_top5_conc': 0.0, 'mean_top10_conc': 0.0, 'min_top5_conc': 0.0}\n",
    "    \n",
    "    top5_concs = []\n",
    "    top10_concs = []\n",
    "    \n",
    "    for score in output.scores:\n",
    "        logits = score[0]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        sorted_probs, _ = torch.sort(probs, descending=True)\n",
    "        \n",
    "        top5_conc = torch.sum(sorted_probs[:5]).item()\n",
    "        top10_conc = torch.sum(sorted_probs[:10]).item()\n",
    "        \n",
    "        top5_concs.append(top5_conc)\n",
    "        top10_concs.append(top10_conc)\n",
    "    \n",
    "    return {\n",
    "        'mean_top5_conc': np.mean(top5_concs),\n",
    "        'mean_top10_conc': np.mean(top10_concs),\n",
    "        'min_top5_conc': np.min(top5_concs)\n",
    "    }\n",
    "\n",
    "def calculate_sequence_metrics(output):\n",
    "    \"\"\"Calculate sequence-level metrics\"\"\"\n",
    "    if not output.scores:\n",
    "        return {'sequence_length': 0, 'entropy_trend': 0.0, 'confidence_trend': 0.0}\n",
    "    \n",
    "    # Get first and last token metrics for trends\n",
    "    first_logits = output.scores[0][0]\n",
    "    last_logits = output.scores[-1][0]\n",
    "    \n",
    "    first_probs = F.softmax(first_logits, dim=-1)\n",
    "    last_probs = F.softmax(last_logits, dim=-1)\n",
    "    \n",
    "    first_entropy = -(first_probs * torch.log(first_probs + 1e-8)).sum().item()\n",
    "    last_entropy = -(last_probs * torch.log(last_probs + 1e-8)).sum().item()\n",
    "    \n",
    "    first_confidence = torch.max(first_probs).item()\n",
    "    last_confidence = torch.max(last_probs).item()\n",
    "    \n",
    "    return {\n",
    "        'sequence_length': len(output.scores),\n",
    "        'entropy_trend': last_entropy - first_entropy,\n",
    "        'confidence_trend': last_confidence - first_confidence\n",
    "    }\n",
    "\n",
    "def calculate_distribution_metrics(output):\n",
    "    \"\"\"Calculate percentile-based distribution metrics\"\"\"\n",
    "    if not output.scores:\n",
    "        return {'entropy_p25': 0.0, 'entropy_p75': 0.0, 'confidence_p25': 0.0, 'confidence_p75': 0.0}\n",
    "    \n",
    "    entropies = []\n",
    "    confidences = []\n",
    "    \n",
    "    for score in output.scores:\n",
    "        logits = score[0]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        entropy = -(probs * torch.log(probs + 1e-8)).sum().item()\n",
    "        confidence = torch.max(probs).item()\n",
    "        \n",
    "        entropies.append(entropy)\n",
    "        confidences.append(confidence)\n",
    "    \n",
    "    return {\n",
    "        'entropy_p25': np.percentile(entropies, 25),\n",
    "        'entropy_p75': np.percentile(entropies, 75),\n",
    "        'confidence_p25': np.percentile(confidences, 25),\n",
    "        'confidence_p75': np.percentile(confidences, 75)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffe741d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_metrics(output):\n",
    "    \"\"\"Combine all uncertainty metrics into single dict\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    metrics.update(calculate_entropy_metrics(output))\n",
    "    metrics.update(calculate_confidence_metrics(output))\n",
    "    metrics.update(calculate_concentration_metrics(output))\n",
    "    metrics.update(calculate_sequence_metrics(output))\n",
    "    metrics.update(calculate_distribution_metrics(output))\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9318336e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qwen_prompt(instruction):\n",
    "    \"\"\"Create structured prompt for Qwen model\"\"\"\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a skilled Python programmer. Write clean, working code without comments or explanations. Focus only on solving the problem correctly.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": f\"Write a Python function to solve this problem. Provide only the function code without comments or explanations:\\n\\n{instruction}\"\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662aa391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_code_from_response(response):\n",
    "    \"\"\"Extract Python code from model response\"\"\"\n",
    "    if \"```python\" in response:\n",
    "        start = response.find(\"```python\") + 9\n",
    "        end = response.find(\"```\", start)\n",
    "        if end != -1:\n",
    "            return response[start:end].strip()\n",
    "    \n",
    "    if \"```\" in response:\n",
    "        start = response.find(\"```\") + 3\n",
    "        end = response.find(\"```\", start)\n",
    "        if end != -1:\n",
    "            return response[start:end].strip()\n",
    "    \n",
    "    lines = response.split('\\n')\n",
    "    code_lines = []\n",
    "    in_function = False\n",
    "    \n",
    "    for line in lines:\n",
    "        if line.strip().startswith('def '):\n",
    "            in_function = True\n",
    "        if in_function:\n",
    "            code_lines.append(line)\n",
    "    \n",
    "    return '\\n'.join(code_lines).strip() if code_lines else response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c6f41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_single_response(model, tokenizer, instruction, seq_id):\n",
    "    \"\"\"Generate code for single problem and extract metrics\"\"\"\n",
    "    try:\n",
    "        messages = create_qwen_prompt(instruction)\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        device = next(model.parameters()).device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        generated_text = tokenizer.decode(\n",
    "            output.sequences[0][inputs['input_ids'].shape[1]:], \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        generated_code = extract_code_from_response(generated_text)\n",
    "        \n",
    "        metrics = extract_all_metrics(output)\n",
    "        \n",
    "        result = {\n",
    "            'seq_id': seq_id,\n",
    "            'instruction': instruction,\n",
    "            'generated_text': generated_text,\n",
    "            'generated_code': generated_code,\n",
    "            'prompt': prompt\n",
    "        }\n",
    "        \n",
    "        result.update(metrics)\n",
    "        \n",
    "        del output, inputs\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating for {seq_id}: {e}\")\n",
    "        \n",
    "        result = {\n",
    "            'seq_id': seq_id,\n",
    "            'instruction': instruction,\n",
    "            'generated_text': '',\n",
    "            'generated_code': '',\n",
    "            'prompt': '',\n",
    "            'error': str(e)\n",
    "        }\n",
    "        \n",
    "        metric_names = ['mean_entropy', 'max_entropy', 'min_entropy', 'std_entropy',\n",
    "                       'mean_confidence', 'min_confidence', 'std_confidence',\n",
    "                       'mean_top5_conc', 'mean_top10_conc', 'min_top5_conc',\n",
    "                       'sequence_length', 'entropy_trend', 'confidence_trend',\n",
    "                       'entropy_p25', 'entropy_p75', 'confidence_p25', 'confidence_p75']\n",
    "        \n",
    "        for metric in metric_names:\n",
    "            result[metric] = 0.0\n",
    "            \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4411ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_code_with_metrics(csv_path, model, tokenizer, output_dir=\"generation_results\", save_every=50):\n",
    "    \"\"\"\n",
    "    Generate code for all problems and extract metrics.\n",
    "    Save progress every N generations.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"Loading dataset...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Loaded {len(df)} problems\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        print(f\"\\nGenerating {idx+1}/{len(df)}: {row['seq_id']}\")\n",
    "        \n",
    "        result = generate_single_response(\n",
    "            model, tokenizer, row['instruction'], row['seq_id']\n",
    "        )\n",
    "        \n",
    "        result.update({\n",
    "            'original_output': row['output'],\n",
    "            'original_code': row['code'],\n",
    "            'entry_point': row['entry_point'],\n",
    "            'testcase': row['testcase']\n",
    "        })\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "        if 'error' not in result:\n",
    "            print(f\"  Entropy: {result['mean_entropy']:.4f}, \"\n",
    "                  f\"Confidence: {result['mean_confidence']:.4f}, \"\n",
    "                  f\"Length: {result['sequence_length']}\")\n",
    "        else:\n",
    "            print(f\"  Error: {result['error']}\")\n",
    "        \n",
    "        if (idx + 1) % save_every == 0 or idx == len(df) - 1:\n",
    "            df_batch = pd.DataFrame(results)\n",
    "            save_path = os.path.join(output_dir, f\"generations_up_to_{idx+1}.csv\")\n",
    "            df_batch.to_csv(save_path, index=False)\n",
    "            print(f\"\\nSaved progress to {save_path}\")\n",
    "            \n",
    "            json_path = os.path.join(output_dir, f\"generations_up_to_{idx+1}.json\")\n",
    "            with open(json_path, 'w') as f:\n",
    "                json.dump(results, f, indent=2)\n",
    "    \n",
    "    final_df = pd.DataFrame(results)\n",
    "    final_path = os.path.join(output_dir, \"all_generations.csv\")\n",
    "    final_df.to_csv(final_path, index=False)\n",
    "    \n",
    "    print(f\"\\n✅ Generation complete! Saved {len(results)} results to {final_path}\")\n",
    "    print(f\"Now you can inspect the data before running expensive testing phase.\")\n",
    "    \n",
    "    # Quick summary\n",
    "    successful = len([r for r in results if 'error' not in r])\n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"- Successful generations: {successful}/{len(results)}\")\n",
    "    print(f\"- Average entropy: {np.mean([r['mean_entropy'] for r in results if 'error' not in r]):.4f}\")\n",
    "    print(f\"- Average confidence: {np.mean([r['mean_confidence'] for r in results if 'error' not in r]):.4f}\")\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f902017",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s\\miniconda3\\envs\\samsungProj\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\s\\miniconda3\\envs\\samsungProj\\Lib\\site-packages\\transformers\\quantizers\\auto.py:226: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Loaded 30 problems\n",
      "\n",
      "Generating 1/30: 660113403\n",
      "  Entropy: 0.0000, Confidence: 1.0000, Length: 36\n",
      "\n",
      "Generating 2/30: 14192481277\n",
      "  Entropy: 0.0118, Confidence: 0.9947, Length: 48\n",
      "\n",
      "Generating 3/30: 70726638201\n",
      "  Entropy: 0.0135, Confidence: 0.9949, Length: 68\n",
      "\n",
      "Generating 4/30: 893855581\n",
      "  Entropy: 0.0418, Confidence: 0.9795, Length: 25\n",
      "\n",
      "Generating 5/30: 20383247274\n",
      "  Entropy: 0.0398, Confidence: 0.9798, Length: 57\n",
      "\n",
      "Generating 6/30: 19434228727\n",
      "  Entropy: 0.0530, Confidence: 0.9744, Length: 112\n",
      "\n",
      "Generating 7/30: 28755799726\n",
      "  Entropy: 0.0109, Confidence: 0.9955, Length: 48\n",
      "\n",
      "Generating 8/30: 70726596281\n",
      "  Entropy: 0.0315, Confidence: 0.9850, Length: 19\n",
      "\n",
      "Generating 9/30: 32279616255\n",
      "  Entropy: 0.0193, Confidence: 0.9917, Length: 28\n",
      "\n",
      "Generating 10/30: 12120976763\n",
      "  Entropy: 0.0123, Confidence: 0.9953, Length: 109\n",
      "\n",
      "Generating 11/30: 18874307847\n",
      "  Entropy: 0.0000, Confidence: 1.0000, Length: 46\n",
      "\n",
      "Generating 12/30: 44289869161\n",
      "  Entropy: 0.0376, Confidence: 0.9822, Length: 99\n",
      "\n",
      "Generating 13/30: 72740153081\n",
      "  Entropy: 0.0387, Confidence: 0.9816, Length: 58\n",
      "\n",
      "Generating 14/30: 25369038617\n",
      "  Entropy: 0.0000, Confidence: 1.0000, Length: 19\n",
      "\n",
      "Generating 15/30: 1281915040\n",
      "  Entropy: 0.0557, Confidence: 0.9714, Length: 61\n",
      "\n",
      "Generating 16/30: 21969432676\n",
      "  Entropy: 0.0280, Confidence: 0.9883, Length: 87\n",
      "\n",
      "Generating 17/30: 24485454232\n",
      "  Entropy: 0.0340, Confidence: 0.9824, Length: 90\n",
      "\n",
      "Generating 18/30: 73965182520\n",
      "  Entropy: 0.0150, Confidence: 0.9919, Length: 43\n",
      "\n",
      "Generating 19/30: 15590275150\n",
      "  Entropy: 0.0371, Confidence: 0.9793, Length: 65\n",
      "\n",
      "Generating 20/30: 74992079799\n",
      "  Entropy: 0.0232, Confidence: 0.9885, Length: 200\n",
      "\n",
      "Generating 21/30: 6257817307\n",
      "  Entropy: 0.0659, Confidence: 0.9661, Length: 23\n",
      "\n",
      "Generating 22/30: 15526957597\n",
      "  Entropy: 0.0259, Confidence: 0.9864, Length: 104\n",
      "\n",
      "Generating 23/30: 20004586671\n",
      "  Entropy: 0.0442, Confidence: 0.9821, Length: 45\n",
      "\n",
      "Generating 24/30: 44911643638\n",
      "  Entropy: 0.0223, Confidence: 0.9889, Length: 151\n",
      "\n",
      "Generating 25/30: 2175400675\n",
      "  Entropy: 0.0291, Confidence: 0.9873, Length: 79\n",
      "\n",
      "Generating 26/30: 40713914324\n",
      "  Entropy: 0.0532, Confidence: 0.9735, Length: 99\n",
      "\n",
      "Generating 27/30: 18779514445\n",
      "  Entropy: 0.0257, Confidence: 0.9887, Length: 78\n",
      "\n",
      "Generating 28/30: 70034082307\n",
      "  Entropy: 0.0100, Confidence: 0.9967, Length: 109\n",
      "\n",
      "Generating 29/30: 74061132546\n",
      "  Entropy: 0.0448, Confidence: 0.9832, Length: 51\n",
      "\n",
      "Generating 30/30: 6536870388\n",
      "  Entropy: 0.0266, Confidence: 0.9886, Length: 57\n",
      "\n",
      "Saved progress to generation_results\\generations_up_to_30.csv\n",
      "\n",
      "✅ Generation complete! Saved 30 results to generation_results\\all_generations.csv\n",
      "Now you can inspect the data before running expensive testing phase.\n",
      "\n",
      "Summary:\n",
      "- Successful generations: 30/30\n",
      "- Average entropy: 0.0284\n",
      "- Average confidence: 0.9866\n",
      "\n",
      "==================================================\n",
      "PHASE 1 COMPLETE: Code generation with metrics\n",
      "==================================================\n",
      "Next steps:\n",
      "1. Inspect the generated code in generation_results/\n",
      "2. Visualize the metrics to understand patterns\n",
      "3. Then run testing phase to create labels\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    \n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    )\n",
    "    model_name = \"unsloth/Qwen2.5-Coder-14B-Instruct-bnb-4bit\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", quantization_config=bnb_config)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    generations_df = generate_code_with_metrics(\n",
    "        csv_path=\"C:/Users/s/Desktop/Dev/SamsungProject/extract/top-30-educational-instruct-rows.csv\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        output_dir=\"generation_results\",\n",
    "        save_every=50  \n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PHASE 1 COMPLETE: Code generation with metrics\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"Next steps:\")\n",
    "    print(\"1. Inspect the generated code in generation_results/\")\n",
    "    print(\"2. Visualize the metrics to understand patterns\")\n",
    "    print(\"3. Then run testing phase to create labels\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084f90fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s\\miniconda3\\envs\\samsungProj\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Limited to 30 samples\n",
      "\n",
      "Generating 1/30: 660113403\n",
      "DEBUG: Generated 36 tokens with scores for 660113403\n",
      "  Sample metrics - Entropy: 0.0428, Confidence: 0.9812\n",
      "  Metrics - Entropy: 0.0428, Confidence: 0.9812, Length: 36\n",
      "\n",
      "Generating 2/30: 14192481277\n",
      "DEBUG: Generated 26 tokens with scores for 14192481277\n",
      "  Sample metrics - Entropy: 0.0654, Confidence: 0.9651\n",
      "  Metrics - Entropy: 0.0654, Confidence: 0.9651, Length: 26\n",
      "\n",
      "Generating 3/30: 70726638201\n",
      "DEBUG: Generated 63 tokens with scores for 70726638201\n",
      "  Sample metrics - Entropy: 0.0165, Confidence: 0.9931\n",
      "  Metrics - Entropy: 0.0165, Confidence: 0.9931, Length: 63\n",
      "\n",
      "Generating 4/30: 893855581\n",
      "DEBUG: Generated 22 tokens with scores for 893855581\n",
      "  Sample metrics - Entropy: 0.0210, Confidence: 0.9921\n",
      "  Metrics - Entropy: 0.0210, Confidence: 0.9921, Length: 22\n",
      "\n",
      "Generating 5/30: 20383247274\n",
      "DEBUG: Generated 49 tokens with scores for 20383247274\n",
      "  Sample metrics - Entropy: 0.1041, Confidence: 0.9497\n",
      "  Metrics - Entropy: 0.1041, Confidence: 0.9497, Length: 49\n",
      "\n",
      "Generating 6/30: 19434228727\n",
      "DEBUG: Generated 115 tokens with scores for 19434228727\n",
      "  Sample metrics - Entropy: 0.0552, Confidence: 0.9735\n",
      "  Metrics - Entropy: 0.0552, Confidence: 0.9735, Length: 115\n",
      "\n",
      "Generating 7/30: 28755799726\n",
      "DEBUG: Generated 48 tokens with scores for 28755799726\n",
      "  Sample metrics - Entropy: 0.0277, Confidence: 0.9832\n",
      "  Metrics - Entropy: 0.0277, Confidence: 0.9832, Length: 48\n",
      "\n",
      "Generating 8/30: 70726596281\n",
      "DEBUG: Generated 19 tokens with scores for 70726596281\n",
      "  Sample metrics - Entropy: 0.0000, Confidence: 1.0000\n",
      "  Metrics - Entropy: 0.0000, Confidence: 1.0000, Length: 19\n",
      "\n",
      "Generating 9/30: 32279616255\n",
      "DEBUG: Generated 28 tokens with scores for 32279616255\n",
      "  Sample metrics - Entropy: 0.0686, Confidence: 0.9706\n",
      "  Metrics - Entropy: 0.0686, Confidence: 0.9706, Length: 28\n",
      "\n",
      "Generating 10/30: 12120976763\n",
      "DEBUG: Generated 90 tokens with scores for 12120976763\n",
      "  Sample metrics - Entropy: 0.0440, Confidence: 0.9808\n",
      "  Metrics - Entropy: 0.0440, Confidence: 0.9808, Length: 90\n",
      "\n",
      "Generating 11/30: 18874307847\n",
      "DEBUG: Generated 46 tokens with scores for 18874307847\n",
      "  Sample metrics - Entropy: 0.0498, Confidence: 0.9797\n",
      "  Metrics - Entropy: 0.0498, Confidence: 0.9797, Length: 46\n",
      "\n",
      "Generating 12/30: 44289869161\n",
      "DEBUG: Generated 119 tokens with scores for 44289869161\n",
      "  Sample metrics - Entropy: 0.0763, Confidence: 0.9630\n",
      "  Metrics - Entropy: 0.0763, Confidence: 0.9630, Length: 119\n",
      "\n",
      "Generating 13/30: 72740153081\n",
      "DEBUG: Generated 155 tokens with scores for 72740153081\n",
      "  Sample metrics - Entropy: 0.1030, Confidence: 0.9549\n",
      "  Metrics - Entropy: 0.1030, Confidence: 0.9549, Length: 155\n",
      "\n",
      "Generating 14/30: 25369038617\n",
      "DEBUG: Generated 19 tokens with scores for 25369038617\n",
      "  Sample metrics - Entropy: 0.0206, Confidence: 0.9930\n",
      "  Metrics - Entropy: 0.0206, Confidence: 0.9930, Length: 19\n",
      "\n",
      "Generating 15/30: 1281915040\n",
      "DEBUG: Generated 105 tokens with scores for 1281915040\n",
      "  Sample metrics - Entropy: 0.0320, Confidence: 0.9862\n",
      "  Metrics - Entropy: 0.0320, Confidence: 0.9862, Length: 105\n",
      "\n",
      "Generating 16/30: 21969432676\n",
      "DEBUG: Generated 90 tokens with scores for 21969432676\n",
      "  Sample metrics - Entropy: 0.0319, Confidence: 0.9836\n",
      "  Metrics - Entropy: 0.0319, Confidence: 0.9836, Length: 90\n",
      "\n",
      "Generating 17/30: 24485454232\n",
      "DEBUG: Generated 95 tokens with scores for 24485454232\n",
      "  Sample metrics - Entropy: 0.0266, Confidence: 0.9875\n",
      "  Metrics - Entropy: 0.0266, Confidence: 0.9875, Length: 95\n",
      "\n",
      "Generating 18/30: 73965182520\n",
      "DEBUG: Generated 38 tokens with scores for 73965182520\n",
      "  Sample metrics - Entropy: 0.0680, Confidence: 0.9669\n",
      "  Metrics - Entropy: 0.0680, Confidence: 0.9669, Length: 38\n",
      "\n",
      "Generating 19/30: 15590275150\n",
      "DEBUG: Generated 59 tokens with scores for 15590275150\n",
      "  Sample metrics - Entropy: 0.0374, Confidence: 0.9856\n",
      "  Metrics - Entropy: 0.0374, Confidence: 0.9856, Length: 59\n",
      "\n",
      "Generating 20/30: 74992079799\n",
      "DEBUG: Generated 73 tokens with scores for 74992079799\n",
      "  Sample metrics - Entropy: 0.0354, Confidence: 0.9862\n",
      "  Metrics - Entropy: 0.0354, Confidence: 0.9862, Length: 73\n",
      "\n",
      "Generating 21/30: 6257817307\n",
      "DEBUG: Generated 21 tokens with scores for 6257817307\n",
      "  Sample metrics - Entropy: 0.1347, Confidence: 0.9328\n",
      "  Metrics - Entropy: 0.1347, Confidence: 0.9328, Length: 21\n",
      "\n",
      "Generating 22/30: 15526957597\n",
      "DEBUG: Generated 84 tokens with scores for 15526957597\n",
      "  Sample metrics - Entropy: 0.0737, Confidence: 0.9687\n",
      "  Metrics - Entropy: 0.0737, Confidence: 0.9687, Length: 84\n",
      "\n",
      "Generating 23/30: 20004586671\n",
      "DEBUG: Generated 47 tokens with scores for 20004586671\n",
      "  Sample metrics - Entropy: 0.0321, Confidence: 0.9850\n",
      "  Metrics - Entropy: 0.0321, Confidence: 0.9850, Length: 47\n",
      "\n",
      "Generating 24/30: 44911643638\n",
      "DEBUG: Generated 144 tokens with scores for 44911643638\n",
      "  Sample metrics - Entropy: 0.0658, Confidence: 0.9698\n",
      "  Metrics - Entropy: 0.0658, Confidence: 0.9698, Length: 144\n",
      "\n",
      "Generating 25/30: 2175400675\n",
      "DEBUG: Generated 72 tokens with scores for 2175400675\n",
      "  Sample metrics - Entropy: 0.0752, Confidence: 0.9690\n",
      "  Metrics - Entropy: 0.0752, Confidence: 0.9690, Length: 72\n",
      "\n",
      "Generating 26/30: 40713914324\n",
      "DEBUG: Generated 102 tokens with scores for 40713914324\n",
      "  Sample metrics - Entropy: 0.0741, Confidence: 0.9631\n",
      "  Metrics - Entropy: 0.0741, Confidence: 0.9631, Length: 102\n",
      "\n",
      "Generating 27/30: 18779514445\n",
      "DEBUG: Generated 82 tokens with scores for 18779514445\n",
      "  Sample metrics - Entropy: 0.0588, Confidence: 0.9711\n",
      "  Metrics - Entropy: 0.0588, Confidence: 0.9711, Length: 82\n",
      "\n",
      "Generating 28/30: 70034082307\n",
      "DEBUG: Generated 108 tokens with scores for 70034082307\n",
      "  Sample metrics - Entropy: 0.0413, Confidence: 0.9816\n",
      "  Metrics - Entropy: 0.0413, Confidence: 0.9816, Length: 108\n",
      "\n",
      "Generating 29/30: 74061132546\n",
      "DEBUG: Generated 59 tokens with scores for 74061132546\n",
      "  Sample metrics - Entropy: 0.0656, Confidence: 0.9718\n",
      "  Metrics - Entropy: 0.0656, Confidence: 0.9718, Length: 59\n",
      "\n",
      "Generating 30/30: 6536870388\n",
      "DEBUG: Generated 47 tokens with scores for 6536870388\n",
      "  Sample metrics - Entropy: 0.0349, Confidence: 0.9864\n",
      "  Metrics - Entropy: 0.0349, Confidence: 0.9864, Length: 47\n",
      "\n",
      "Saved progress to generation_results\\generations_up_to_30.csv\n",
      "\n",
      "✅ Generation complete! Saved 30 results to generation_results\\all_generations.csv\n",
      "\n",
      "Summary:\n",
      "- Successful generations: 29/30\n",
      "- Average entropy: 0.0546\n",
      "- Average confidence: 0.9750\n",
      "- Entropy range: 0.0165 - 0.1347\n",
      "\n",
      "==================================================\n",
      "PHASE 1 COMPLETE: Code generation with metrics\n",
      "==================================================\n",
      "Next steps:\n",
      "1. Inspect the generated code in generation_results/\n",
      "2. Visualize the metrics to understand patterns\n",
      "3. Then run testing phase to create labels\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_entropy_metrics(output):\n",
    "    \"\"\"Calculate entropy-based uncertainty metrics - FIXED\"\"\"\n",
    "    if not hasattr(output, 'scores') or not output.scores:\n",
    "        return {'mean_entropy': 0.0, 'max_entropy': 0.0, 'min_entropy': 0.0, 'std_entropy': 0.0}\n",
    "    \n",
    "    entropies = []\n",
    "    for score in output.scores:\n",
    "        \n",
    "        \n",
    "        logits = score[0]  \n",
    "        \n",
    "        \n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        entropy = -(probs * torch.log(probs + 1e-8)).sum().item()\n",
    "        entropies.append(entropy)\n",
    "    \n",
    "    if not entropies:\n",
    "        return {'mean_entropy': 0.0, 'max_entropy': 0.0, 'min_entropy': 0.0, 'std_entropy': 0.0}\n",
    "    \n",
    "    return {\n",
    "        'mean_entropy': np.mean(entropies),\n",
    "        'max_entropy': np.max(entropies),\n",
    "        'min_entropy': np.min(entropies),\n",
    "        'std_entropy': np.std(entropies)\n",
    "    }\n",
    "\n",
    "def calculate_confidence_metrics(output):\n",
    "    \"\"\"Calculate confidence-based metrics - FIXED\"\"\"\n",
    "    if not hasattr(output, 'scores') or not output.scores:\n",
    "        return {'mean_confidence': 0.0, 'min_confidence': 0.0, 'std_confidence': 0.0}\n",
    "    \n",
    "    confidences = []\n",
    "    for score in output.scores:\n",
    "        logits = score[0]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        max_prob = torch.max(probs).item()\n",
    "        confidences.append(max_prob)\n",
    "    \n",
    "    if not confidences:\n",
    "        return {'mean_confidence': 0.0, 'min_confidence': 0.0, 'std_confidence': 0.0}\n",
    "    \n",
    "    return {\n",
    "        'mean_confidence': np.mean(confidences),\n",
    "        'min_confidence': np.min(confidences),\n",
    "        'std_confidence': np.std(confidences)\n",
    "    }\n",
    "\n",
    "def calculate_concentration_metrics(output):\n",
    "    \"\"\"Calculate top-k concentration metrics - FIXED\"\"\"\n",
    "    if not hasattr(output, 'scores') or not output.scores:\n",
    "        return {'mean_top5_conc': 0.0, 'mean_top10_conc': 0.0, 'min_top5_conc': 0.0}\n",
    "    \n",
    "    top5_concs = []\n",
    "    top10_concs = []\n",
    "    \n",
    "    for score in output.scores:\n",
    "        logits = score[0]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        sorted_probs, _ = torch.sort(probs, descending=True)\n",
    "        \n",
    "        top5_conc = torch.sum(sorted_probs[:5]).item()\n",
    "        top10_conc = torch.sum(sorted_probs[:10]).item()\n",
    "        \n",
    "        top5_concs.append(top5_conc)\n",
    "        top10_concs.append(top10_conc)\n",
    "    \n",
    "    if not top5_concs:\n",
    "        return {'mean_top5_conc': 0.0, 'mean_top10_conc': 0.0, 'min_top5_conc': 0.0}\n",
    "    \n",
    "    return {\n",
    "        'mean_top5_conc': np.mean(top5_concs),\n",
    "        'mean_top10_conc': np.mean(top10_concs),\n",
    "        'min_top5_conc': np.min(top5_concs)\n",
    "    }\n",
    "\n",
    "def calculate_sequence_metrics(output):\n",
    "    \"\"\"Calculate sequence-level metrics - FIXED\"\"\"\n",
    "    if not hasattr(output, 'scores') or not output.scores or len(output.scores) < 2:\n",
    "        return {'sequence_length': 0, 'entropy_trend': 0.0, 'confidence_trend': 0.0}\n",
    "    \n",
    "    \n",
    "    first_logits = output.scores[0][0]\n",
    "    last_logits = output.scores[-1][0]\n",
    "    \n",
    "    first_probs = F.softmax(first_logits, dim=-1)\n",
    "    last_probs = F.softmax(last_logits, dim=-1)\n",
    "    \n",
    "    first_entropy = -(first_probs * torch.log(first_probs + 1e-8)).sum().item()\n",
    "    last_entropy = -(last_probs * torch.log(last_probs + 1e-8)).sum().item()\n",
    "    \n",
    "    first_confidence = torch.max(first_probs).item()\n",
    "    last_confidence = torch.max(last_probs).item()\n",
    "    \n",
    "    return {\n",
    "        'sequence_length': len(output.scores),\n",
    "        'entropy_trend': last_entropy - first_entropy,\n",
    "        'confidence_trend': last_confidence - first_confidence\n",
    "    }\n",
    "\n",
    "def calculate_distribution_metrics(output):\n",
    "    \"\"\"Calculate percentile-based distribution metrics - FIXED\"\"\"\n",
    "    if not hasattr(output, 'scores') or not output.scores:\n",
    "        return {'entropy_p25': 0.0, 'entropy_p75': 0.0, 'confidence_p25': 0.0, 'confidence_p75': 0.0}\n",
    "    \n",
    "    entropies = []\n",
    "    confidences = []\n",
    "    \n",
    "    for score in output.scores:\n",
    "        logits = score[0]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        entropy = -(probs * torch.log(probs + 1e-8)).sum().item()\n",
    "        confidence = torch.max(probs).item()\n",
    "        \n",
    "        entropies.append(entropy)\n",
    "        confidences.append(confidence)\n",
    "    \n",
    "    if not entropies:\n",
    "        return {'entropy_p25': 0.0, 'entropy_p75': 0.0, 'confidence_p25': 0.0, 'confidence_p75': 0.0}\n",
    "    \n",
    "    return {\n",
    "        'entropy_p25': np.percentile(entropies, 25),\n",
    "        'entropy_p75': np.percentile(entropies, 75),\n",
    "        'confidence_p25': np.percentile(confidences, 25),\n",
    "        'confidence_p75': np.percentile(confidences, 75)\n",
    "    }\n",
    "\n",
    "def extract_all_metrics(output):\n",
    "    \"\"\"Combine all uncertainty metrics into single dict\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    \n",
    "    metrics.update(calculate_entropy_metrics(output))\n",
    "    metrics.update(calculate_confidence_metrics(output))\n",
    "    metrics.update(calculate_concentration_metrics(output))\n",
    "    metrics.update(calculate_sequence_metrics(output))\n",
    "    metrics.update(calculate_distribution_metrics(output))\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_single_response(model, tokenizer, instruction, seq_id):\n",
    "    \"\"\"Generate code for single problem and extract metrics - IMPROVED\"\"\"\n",
    "    try:\n",
    "        \n",
    "        messages = create_qwen_prompt(instruction)\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        device = next(model.parameters()).device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "                do_sample=True,  \n",
    "                temperature=0.8,  \n",
    "                top_p=0.9,\n",
    "                top_k=50,  \n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                \n",
    "                num_beams=1,  \n",
    "            )\n",
    "        \n",
    "        \n",
    "        if not hasattr(output, 'scores') or not output.scores:\n",
    "            print(f\"WARNING: No scores returned for {seq_id}\")\n",
    "            return create_empty_result(seq_id, instruction, \"No scores returned\")\n",
    "        \n",
    "        print(f\"DEBUG: Generated {len(output.scores)} tokens with scores for {seq_id}\")\n",
    "        \n",
    "        \n",
    "        generated_text = tokenizer.decode(\n",
    "            output.sequences[0][inputs['input_ids'].shape[1]:], \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        generated_code = extract_code_from_response(generated_text)\n",
    "        \n",
    "        \n",
    "        metrics = extract_all_metrics(output)\n",
    "        \n",
    "        \n",
    "        print(f\"  Sample metrics - Entropy: {metrics.get('mean_entropy', 0):.4f}, \"\n",
    "              f\"Confidence: {metrics.get('mean_confidence', 0):.4f}\")\n",
    "        \n",
    "        \n",
    "        result = {\n",
    "            'seq_id': seq_id,\n",
    "            'instruction': instruction,\n",
    "            'generated_text': generated_text,\n",
    "            'generated_code': generated_code,\n",
    "            'prompt': prompt\n",
    "        }\n",
    "        \n",
    "        \n",
    "        result.update(metrics)\n",
    "        \n",
    "        \n",
    "        del output, inputs\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating for {seq_id}: {e}\")\n",
    "        return create_empty_result(seq_id, instruction, str(e))\n",
    "\n",
    "def create_empty_result(seq_id, instruction, error_msg):\n",
    "    \"\"\"Create empty result with zero metrics\"\"\"\n",
    "    result = {\n",
    "        'seq_id': seq_id,\n",
    "        'instruction': instruction,\n",
    "        'generated_text': '',\n",
    "        'generated_code': '',\n",
    "        'prompt': '',\n",
    "        'error': error_msg\n",
    "    }\n",
    "    \n",
    "    \n",
    "    metric_names = ['mean_entropy', 'max_entropy', 'min_entropy', 'std_entropy',\n",
    "                   'mean_confidence', 'min_confidence', 'std_confidence',\n",
    "                   'mean_top5_conc', 'mean_top10_conc', 'min_top5_conc',\n",
    "                   'sequence_length', 'entropy_trend', 'confidence_trend',\n",
    "                   'entropy_p25', 'entropy_p75', 'confidence_p25', 'confidence_p75']\n",
    "    \n",
    "    for metric in metric_names:\n",
    "        result[metric] = 0.0\n",
    "        \n",
    "    return result\n",
    "\n",
    "def create_qwen_prompt(instruction):\n",
    "    \"\"\"Create structured prompt for Qwen model\"\"\"\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a skilled Python programmer. Write clean, working code without comments or explanations. Focus only on solving the problem correctly.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": f\"Write a Python function to solve this problem. Provide only the function code without comments or explanations:\\n\\n{instruction}\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "def extract_code_from_response(response):\n",
    "    \"\"\"Extract Python code from model response\"\"\"\n",
    "    \n",
    "    if \"```python\" in response:\n",
    "        start = response.find(\"```python\") + 9\n",
    "        end = response.find(\"```\", start)\n",
    "        if end != -1:\n",
    "            return response[start:end].strip()\n",
    "    \n",
    "    if \"```\" in response:\n",
    "        start = response.find(\"```\") + 3\n",
    "        end = response.find(\"```\", start)\n",
    "        if end != -1:\n",
    "            return response[start:end].strip()\n",
    "    \n",
    "    \n",
    "    lines = response.split('\\n')\n",
    "    code_lines = []\n",
    "    in_function = False\n",
    "    \n",
    "    for line in lines:\n",
    "        if line.strip().startswith('def '):\n",
    "            in_function = True\n",
    "        if in_function:\n",
    "            code_lines.append(line)\n",
    "    \n",
    "    return '\\n'.join(code_lines).strip() if code_lines else response.strip()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_code_with_metrics(csv_path, model, tokenizer, output_dir=\"generation_results\", save_every=50, max_samples=None):\n",
    "    \"\"\"\n",
    "    Generate code for all problems and extract metrics.\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to input CSV\n",
    "        model: Your language model\n",
    "        tokenizer: Your tokenizer\n",
    "        output_dir: Directory to save results\n",
    "        save_every: Save progress every N generations\n",
    "        max_samples: Limit number of samples (None for all)\n",
    "    \"\"\"\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    \n",
    "    print(\"Loading dataset...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    if max_samples:\n",
    "        df = df.head(max_samples)\n",
    "        print(f\"Limited to {len(df)} samples\")\n",
    "    else:\n",
    "        print(f\"Loaded {len(df)} problems\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        print(f\"\\nGenerating {idx+1}/{len(df)}: {row['seq_id']}\")\n",
    "        \n",
    "        \n",
    "        result = generate_single_response(\n",
    "            model, tokenizer, row['instruction'], row['seq_id']\n",
    "        )\n",
    "        \n",
    "        \n",
    "        result.update({\n",
    "            'original_output': row['output'],\n",
    "            'original_code': row['code'],\n",
    "            'entry_point': row['entry_point'],\n",
    "            'testcase': row['testcase']\n",
    "        })\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "        \n",
    "        if 'error' not in result:\n",
    "            print(f\"  Metrics - Entropy: {result['mean_entropy']:.4f}, \"\n",
    "                  f\"Confidence: {result['mean_confidence']:.4f}, \"\n",
    "                  f\"Length: {result['sequence_length']}\")\n",
    "        else:\n",
    "            print(f\"  Error: {result['error']}\")\n",
    "        \n",
    "        \n",
    "        if (idx + 1) % save_every == 0 or idx == len(df) - 1:\n",
    "            df_batch = pd.DataFrame(results)\n",
    "            save_path = os.path.join(output_dir, f\"generations_up_to_{idx+1}.csv\")\n",
    "            df_batch.to_csv(save_path, index=False)\n",
    "            print(f\"\\nSaved progress to {save_path}\")\n",
    "    \n",
    "    \n",
    "    final_df = pd.DataFrame(results)\n",
    "    final_path = os.path.join(output_dir, \"all_generations.csv\")\n",
    "    final_df.to_csv(final_path, index=False)\n",
    "    \n",
    "    print(f\"\\n✅ Generation complete! Saved {len(results)} results to {final_path}\")\n",
    "    \n",
    "    \n",
    "    successful = len([r for r in results if 'error' not in r and r['mean_entropy'] > 0])\n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"- Successful generations: {successful}/{len(results)}\")\n",
    "    \n",
    "    if successful > 0:\n",
    "        valid_results = [r for r in results if 'error' not in r and r['mean_entropy'] > 0]\n",
    "        print(f\"- Average entropy: {np.mean([r['mean_entropy'] for r in valid_results]):.4f}\")\n",
    "        print(f\"- Average confidence: {np.mean([r['mean_confidence'] for r in valid_results]):.4f}\")\n",
    "        print(f\"- Entropy range: {np.min([r['mean_entropy'] for r in valid_results]):.4f} - {np.max([r['mean_entropy'] for r in valid_results]):.4f}\")\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    \n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    )\n",
    "    model_name = \"unsloth/Qwen2.5-Coder-7B-Instruct-bnb-4bit\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    \n",
    "    generations_df = generate_code_with_metrics(\n",
    "        csv_path=\"C:/Users/s/Desktop/Dev/SamsungProject/extract/top-30-educational-instruct-rows.csv\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        output_dir=\"generation_results\",\n",
    "        save_every=50,  \n",
    "        max_samples=100\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PHASE 1 COMPLETE: Code generation with metrics\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"Next steps:\")\n",
    "    print(\"1. Inspect the generated code in generation_results/\")\n",
    "    print(\"2. Visualize the metrics to understand patterns\")\n",
    "    print(\"3. Then run testing phase to create labels\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fe2dbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: unsloth/Qwen2.5-Coder-7B-Instruct-bnb-4bit\n",
      "Model loaded successfully!\n",
      "============================================================\n",
      "STARTING PHASE 1: Code generation with calibrated uncertainty metrics\n",
      "============================================================\n",
      "Loading dataset...\n",
      "Limited to 30 samples\n",
      "\n",
      "Generating 1/30: 660113403\n",
      "DEBUG: Generated 43 tokens with scores for 660113403\n",
      "  Original  - Entropy: 0.0371, Confidence: 0.9833\n",
      "  Calibrated- Entropy: 0.0424, Confidence: 0.9796\n",
      "  Advanced  - Token Disagreement: 18.3773, Mutual Info: 0.0000\n",
      "  Final Summary - Original Entropy: 0.0371, Calibrated Entropy: 0.0424\n",
      "                  Original Confidence: 0.9833, Calibrated Confidence: 0.9796\n",
      "\n",
      "Generating 2/30: 14192481277\n",
      "DEBUG: Generated 26 tokens with scores for 14192481277\n",
      "  Original  - Entropy: 0.0680, Confidence: 0.9666\n",
      "  Calibrated- Entropy: 0.0742, Confidence: 0.9593\n",
      "  Advanced  - Token Disagreement: 18.3435, Mutual Info: 0.0000\n",
      "  Final Summary - Original Entropy: 0.0680, Calibrated Entropy: 0.0742\n",
      "                  Original Confidence: 0.9666, Calibrated Confidence: 0.9593\n",
      "\n",
      "Generating 3/30: 70726638201\n",
      "DEBUG: Generated 63 tokens with scores for 70726638201\n",
      "  Original  - Entropy: 0.0164, Confidence: 0.9926\n",
      "  Calibrated- Entropy: 0.0191, Confidence: 0.9903\n",
      "  Advanced  - Token Disagreement: 18.4013, Mutual Info: 0.0000\n",
      "  Final Summary - Original Entropy: 0.0164, Calibrated Entropy: 0.0191\n",
      "                  Original Confidence: 0.9926, Calibrated Confidence: 0.9903\n",
      "\n",
      "Generating 4/30: 893855581\n",
      "DEBUG: Generated 28 tokens with scores for 893855581\n",
      "  Original  - Entropy: 0.0770, Confidence: 0.9665\n",
      "  Calibrated- Entropy: 0.0882, Confidence: 0.9555\n",
      "  Advanced  - Token Disagreement: 18.3293, Mutual Info: 0.0000\n",
      "  Final Summary - Original Entropy: 0.0770, Calibrated Entropy: 0.0882\n",
      "                  Original Confidence: 0.9665, Calibrated Confidence: 0.9555\n",
      "\n",
      "Generating 5/30: 20383247274\n",
      "DEBUG: Generated 62 tokens with scores for 20383247274\n",
      "  Original  - Entropy: 0.1273, Confidence: 0.9375\n",
      "  Calibrated- Entropy: 0.1405, Confidence: 0.9252\n",
      "  Advanced  - Token Disagreement: 18.2779, Mutual Info: 0.0000\n",
      "  Final Summary - Original Entropy: 0.1273, Calibrated Entropy: 0.1405\n",
      "                  Original Confidence: 0.9375, Calibrated Confidence: 0.9252\n",
      "\n",
      "Generating 6/30: 19434228727\n",
      "DEBUG: Generated 109 tokens with scores for 19434228727\n",
      "  Original  - Entropy: 0.0491, Confidence: 0.9765\n",
      "  Calibrated- Entropy: 0.0548, Confidence: 0.9711\n",
      "  Advanced  - Token Disagreement: 18.3653, Mutual Info: 0.0000\n",
      "  Final Summary - Original Entropy: 0.0491, Calibrated Entropy: 0.0548\n",
      "                  Original Confidence: 0.9765, Calibrated Confidence: 0.9711\n",
      "\n",
      "Generating 7/30: 28755799726\n",
      "DEBUG: Generated 48 tokens with scores for 28755799726\n",
      "  Original  - Entropy: 0.0369, Confidence: 0.9790\n",
      "  Calibrated- Entropy: 0.0400, Confidence: 0.9762\n",
      "  Advanced  - Token Disagreement: 18.3799, Mutual Info: 0.0000\n",
      "  Final Summary - Original Entropy: 0.0369, Calibrated Entropy: 0.0400\n",
      "                  Original Confidence: 0.9790, Calibrated Confidence: 0.9762\n",
      "\n",
      "Generating 8/30: 70726596281\n",
      "DEBUG: Generated 19 tokens with scores for 70726596281\n",
      "  Original  - Entropy: 0.0493, Confidence: 0.9813\n",
      "  Calibrated- Entropy: 0.0609, Confidence: 0.9721\n",
      "  Advanced  - Token Disagreement: 18.0809, Mutual Info: 0.0000\n",
      "  Final Summary - Original Entropy: 0.0493, Calibrated Entropy: 0.0609\n",
      "                  Original Confidence: 0.9813, Calibrated Confidence: 0.9721\n",
      "\n",
      "Generating 9/30: 32279616255\n",
      "DEBUG: Generated 40 tokens with scores for 32279616255\n",
      "  Original  - Entropy: 0.0704, Confidence: 0.9654\n",
      "  Calibrated- Entropy: 0.0782, Confidence: 0.9576\n",
      "  Advanced  - Token Disagreement: 18.2449, Mutual Info: 0.0000\n",
      "  Final Summary - Original Entropy: 0.0704, Calibrated Entropy: 0.0782\n",
      "                  Original Confidence: 0.9654, Calibrated Confidence: 0.9576\n",
      "\n",
      "Generating 10/30: 12120976763\n",
      "DEBUG: Generated 90 tokens with scores for 12120976763\n",
      "  Original  - Entropy: 0.0672, Confidence: 0.9728\n",
      "  Calibrated- Entropy: 0.0765, Confidence: 0.9674\n",
      "  Advanced  - Token Disagreement: 18.3433, Mutual Info: 0.0000\n",
      "  Final Summary - Original Entropy: 0.0672, Calibrated Entropy: 0.0765\n",
      "                  Original Confidence: 0.9728, Calibrated Confidence: 0.9674\n",
      "\n",
      "Generating 11/30: 18874307847\n",
      "DEBUG: Generated 46 tokens with scores for 18874307847\n",
      "  Original  - Entropy: 0.0687, Confidence: 0.9688\n",
      "  Calibrated- Entropy: 0.0766, Confidence: 0.9609\n",
      "  Advanced  - Token Disagreement: 18.3424, Mutual Info: 0.0000\n",
      "  Final Summary - Original Entropy: 0.0687, Calibrated Entropy: 0.0766\n",
      "                  Original Confidence: 0.9688, Calibrated Confidence: 0.9609\n",
      "\n",
      "Generating 12/30: 44289869161\n",
      "DEBUG: Generated 98 tokens with scores for 44289869161\n",
      "  Original  - Entropy: 0.0856, Confidence: 0.9546\n",
      "  Calibrated- Entropy: 0.0908, Confidence: 0.9495\n",
      "  Advanced  - Token Disagreement: 18.1942, Mutual Info: 0.0000\n",
      "  Final Summary - Original Entropy: 0.0856, Calibrated Entropy: 0.0908\n",
      "                  Original Confidence: 0.9546, Calibrated Confidence: 0.9495\n",
      "\n",
      "Generating 13/30: 72740153081\n",
      "DEBUG: Generated 62 tokens with scores for 72740153081\n",
      "  Original  - Entropy: 0.0381, Confidence: 0.9823\n",
      "  Calibrated- Entropy: 0.0445, Confidence: 0.9775\n",
      "  Advanced  - Token Disagreement: 18.3754, Mutual Info: 0.0000\n",
      "  Final Summary - Original Entropy: 0.0381, Calibrated Entropy: 0.0445\n",
      "                  Original Confidence: 0.9823, Calibrated Confidence: 0.9775\n",
      "\n",
      "Generating 14/30: 25369038617\n",
      "DEBUG: Generated 15 tokens with scores for 25369038617\n",
      "  Original  - Entropy: 0.0354, Confidence: 0.9852\n",
      "  Calibrated- Entropy: 0.0409, Confidence: 0.9798\n",
      "  Advanced  - Token Disagreement: 18.3769, Mutual Info: 0.0000\n",
      "  Final Summary - Original Entropy: 0.0354, Calibrated Entropy: 0.0409\n",
      "                  Original Confidence: 0.9852, Calibrated Confidence: 0.9798\n",
      "\n",
      "Generating 15/30: 1281915040\n",
      "DEBUG: Generated 43 tokens with scores for 1281915040\n",
      "  Original  - Entropy: 0.0798, Confidence: 0.9642\n",
      "  Calibrated- Entropy: 0.0884, Confidence: 0.9557\n",
      "  Advanced  - Token Disagreement: 18.3301, Mutual Info: 0.0000\n",
      "  Final Summary - Original Entropy: 0.0798, Calibrated Entropy: 0.0884\n",
      "                  Original Confidence: 0.9642, Calibrated Confidence: 0.9557\n",
      "\n",
      "Generating 16/30: 21969432676\n",
      "DEBUG: Generated 72 tokens with scores for 21969432676\n",
      "  Original  - Entropy: 0.0742, Confidence: 0.9677\n",
      "  Calibrated- Entropy: 0.0855, Confidence: 0.9594\n",
      "  Advanced  - Token Disagreement: 18.3339, Mutual Info: 0.0000\n",
      "  Final Summary - Original Entropy: 0.0742, Calibrated Entropy: 0.0855\n",
      "                  Original Confidence: 0.9677, Calibrated Confidence: 0.9594\n",
      "\n",
      "Generating 17/30: 24485454232\n",
      "DEBUG: Generated 57 tokens with scores for 24485454232\n",
      "  Original  - Entropy: 0.0468, Confidence: 0.9765\n",
      "  Calibrated- Entropy: 0.0511, Confidence: 0.9723\n",
      "  Advanced  - Token Disagreement: 18.3687, Mutual Info: 0.0000\n",
      "  Final Summary - Original Entropy: 0.0468, Calibrated Entropy: 0.0511\n",
      "                  Original Confidence: 0.9765, Calibrated Confidence: 0.9723\n",
      "\n",
      "Generating 18/30: 73965182520\n",
      "DEBUG: Generated 41 tokens with scores for 73965182520\n",
      "  Original  - Entropy: 0.0863, Confidence: 0.9582\n",
      "  Calibrated- Entropy: 0.0940, Confidence: 0.9505\n",
      "  Advanced  - Token Disagreement: 18.3243, Mutual Info: 0.0000\n",
      "  Final Summary - Original Entropy: 0.0863, Calibrated Entropy: 0.0940\n",
      "                  Original Confidence: 0.9582, Calibrated Confidence: 0.9505\n",
      "\n",
      "Generating 19/30: 15590275150\n",
      "DEBUG: Generated 58 tokens with scores for 15590275150\n",
      "  Original  - Entropy: 0.0493, Confidence: 0.9802\n",
      "  Calibrated- Entropy: 0.0598, Confidence: 0.9718\n",
      "  Advanced  - Token Disagreement: 18.3598, Mutual Info: 0.0000\n",
      "  Final Summary - Original Entropy: 0.0493, Calibrated Entropy: 0.0598\n",
      "                  Original Confidence: 0.9802, Calibrated Confidence: 0.9718\n",
      "\n",
      "Generating 20/30: 74992079799\n",
      "DEBUG: Generated 77 tokens with scores for 74992079799\n",
      "  Original  - Entropy: 0.0494, Confidence: 0.9777\n",
      "  Calibrated- Entropy: 0.0544, Confidence: 0.9734\n",
      "  Advanced  - Token Disagreement: 18.3656, Mutual Info: 0.0000\n",
      "  Final Summary - Original Entropy: 0.0494, Calibrated Entropy: 0.0544\n",
      "                  Original Confidence: 0.9777, Calibrated Confidence: 0.9734\n",
      "\n",
      "Generating 21/30: 6257817307\n",
      "DEBUG: Generated 25 tokens with scores for 6257817307\n",
      "  Original  - Entropy: 0.1028, Confidence: 0.9486\n",
      "  Calibrated- Entropy: 0.1093, Confidence: 0.9421\n",
      "  Advanced  - Token Disagreement: 18.3069, Mutual Info: 0.0000\n",
      "  Final Summary - Original Entropy: 0.1028, Calibrated Entropy: 0.1093\n",
      "                  Original Confidence: 0.9486, Calibrated Confidence: 0.9421\n",
      "\n",
      "Generating 22/30: 15526957597\n",
      "DEBUG: Generated 39 tokens with scores for 15526957597\n",
      "  Original  - Entropy: 0.1773, Confidence: 0.9180\n",
      "  Calibrated- Entropy: 0.1919, Confidence: 0.9018\n",
      "  Advanced  - Token Disagreement: 18.2238, Mutual Info: 0.0000\n",
      "  Final Summary - Original Entropy: 0.1773, Calibrated Entropy: 0.1919\n",
      "                  Original Confidence: 0.9180, Calibrated Confidence: 0.9018\n",
      "\n",
      "Generating 23/30: 20004586671\n",
      "DEBUG: Generated 47 tokens with scores for 20004586671\n",
      "  Original  - Entropy: 0.0495, Confidence: 0.9728\n",
      "  Calibrated- Entropy: 0.0541, Confidence: 0.9685\n",
      "  Advanced  - Token Disagreement: 18.3654, Mutual Info: 0.0000\n",
      "  Final Summary - Original Entropy: 0.0495, Calibrated Entropy: 0.0541\n",
      "                  Original Confidence: 0.9728, Calibrated Confidence: 0.9685\n",
      "\n",
      "Generating 24/30: 44911643638\n",
      "DEBUG: Generated 143 tokens with scores for 44911643638\n",
      "  Original  - Entropy: 0.0498, Confidence: 0.9757\n",
      "  Calibrated- Entropy: 0.0548, Confidence: 0.9712\n",
      "  Advanced  - Token Disagreement: 18.3655, Mutual Info: 0.0000\n",
      "  Final Summary - Original Entropy: 0.0498, Calibrated Entropy: 0.0548\n",
      "                  Original Confidence: 0.9757, Calibrated Confidence: 0.9712\n",
      "\n",
      "Generating 25/30: 2175400675\n",
      "DEBUG: Generated 75 tokens with scores for 2175400675\n",
      "  Original  - Entropy: 0.0604, Confidence: 0.9744\n",
      "  Calibrated- Entropy: 0.0713, Confidence: 0.9650\n",
      "  Advanced  - Token Disagreement: 18.3484, Mutual Info: 0.0000\n",
      "  Final Summary - Original Entropy: 0.0604, Calibrated Entropy: 0.0713\n",
      "                  Original Confidence: 0.9744, Calibrated Confidence: 0.9650\n",
      "\n",
      "Saved progress to generation_results_calibrated\\generations_up_to_25.csv\n",
      "\n",
      "Generating 26/30: 40713914324\n",
      "DEBUG: Generated 108 tokens with scores for 40713914324\n",
      "  Original  - Entropy: 0.0612, Confidence: 0.9703\n",
      "  Calibrated- Entropy: 0.0669, Confidence: 0.9646\n",
      "  Advanced  - Token Disagreement: 18.2823, Mutual Info: 0.0000\n",
      "  Final Summary - Original Entropy: 0.0612, Calibrated Entropy: 0.0669\n",
      "                  Original Confidence: 0.9703, Calibrated Confidence: 0.9646\n",
      "\n",
      "Generating 27/30: 18779514445\n",
      "DEBUG: Generated 67 tokens with scores for 18779514445\n",
      "  Original  - Entropy: 0.0496, Confidence: 0.9765\n",
      "  Calibrated- Entropy: 0.0556, Confidence: 0.9716\n",
      "  Advanced  - Token Disagreement: 18.3643, Mutual Info: 0.0000\n",
      "  Final Summary - Original Entropy: 0.0496, Calibrated Entropy: 0.0556\n",
      "                  Original Confidence: 0.9765, Calibrated Confidence: 0.9716\n",
      "\n",
      "Generating 28/30: 70034082307\n",
      "DEBUG: Generated 112 tokens with scores for 70034082307\n",
      "  Original  - Entropy: 0.0542, Confidence: 0.9768\n",
      "  Calibrated- Entropy: 0.0646, Confidence: 0.9694\n",
      "  Advanced  - Token Disagreement: 18.3555, Mutual Info: 0.0000\n",
      "  Final Summary - Original Entropy: 0.0542, Calibrated Entropy: 0.0646\n",
      "                  Original Confidence: 0.9768, Calibrated Confidence: 0.9694\n",
      "\n",
      "Generating 29/30: 74061132546\n",
      "DEBUG: Generated 36 tokens with scores for 74061132546\n",
      "  Original  - Entropy: 0.1008, Confidence: 0.9529\n",
      "  Calibrated- Entropy: 0.1110, Confidence: 0.9440\n",
      "  Advanced  - Token Disagreement: 18.3065, Mutual Info: 0.0000\n",
      "  Final Summary - Original Entropy: 0.1008, Calibrated Entropy: 0.1110\n",
      "                  Original Confidence: 0.9529, Calibrated Confidence: 0.9440\n",
      "\n",
      "Generating 30/30: 6536870388\n",
      "DEBUG: Generated 45 tokens with scores for 6536870388\n",
      "  Original  - Entropy: 0.0447, Confidence: 0.9802\n",
      "  Calibrated- Entropy: 0.0500, Confidence: 0.9752\n",
      "  Advanced  - Token Disagreement: 18.3695, Mutual Info: 0.0000\n",
      "  Final Summary - Original Entropy: 0.0447, Calibrated Entropy: 0.0500\n",
      "                  Original Confidence: 0.9802, Calibrated Confidence: 0.9752\n",
      "\n",
      "Saved progress to generation_results_calibrated\\generations_up_to_30.csv\n",
      "\n",
      "✅ Generation complete! Saved 30 results to generation_results_calibrated\\all_generations_with_calibrated_metrics.csv\n",
      "\n",
      "Summary:\n",
      "- Successful generations: 30/30\n",
      "\n",
      "Original Metrics:\n",
      "- Average entropy: 0.0654\n",
      "- Average confidence: 0.9694\n",
      "- Entropy range: 0.0164 - 0.1773\n",
      "\n",
      "Calibrated Metrics:\n",
      "- Average calibrated entropy: 0.0730\n",
      "- Average calibrated confidence: 0.9626\n",
      "- Average token disagreement: 18.3268\n",
      "- Average mutual information: 0.0000\n",
      "- Calibrated entropy range: 0.0191 - 0.1919\n",
      "\n",
      "============================================================\n",
      "PHASE 1 COMPLETE: Code generation with calibrated uncertainty metrics\n",
      "============================================================\n",
      "Generated files:\n",
      "- all_generations_with_calibrated_metrics.csv\n",
      "\n",
      "Key improvements:\n",
      "✅ Temperature calibration (1.2 generation, 1.5 post-processing)\n",
      "✅ Token disagreement metrics\n",
      "✅ Mutual information uncertainty\n",
      "✅ Semantic uncertainty (position-aware)\n",
      "✅ Better sampling parameters\n",
      "\n",
      "Next steps:\n",
      "1. Inspect the calibrated metrics - should see more variation!\n",
      "2. Compare original vs calibrated uncertainty distributions\n",
      "3. Run testing phase to create labels using Pass@1\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# =============================================================================\n",
    "# CALIBRATED UNCERTAINTY METRICS FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_calibrated_uncertainty_metrics(output, temperature_param=1.5):\n",
    "    \"\"\"\n",
    "    Calculate calibrated uncertainty metrics to address overconfidence.\n",
    "    \n",
    "    Args:\n",
    "        output: Model output with scores\n",
    "        temperature_param: Temperature for calibration (higher = less confident)\n",
    "    \n",
    "    Returns:\n",
    "        Dict with calibrated uncertainty metrics\n",
    "    \"\"\"\n",
    "    if not hasattr(output, 'scores') or not output.scores:\n",
    "        return get_empty_calibrated_metrics()\n",
    "    \n",
    "    calibrated_entropies = []\n",
    "    calibrated_confidences = []\n",
    "    token_disagreements = []\n",
    "    prob_concentrations = []\n",
    "    semantic_uncertainties = []\n",
    "    \n",
    "    for i, score in enumerate(output.scores):\n",
    "        logits = score[0]  # Shape: (vocab_size,)\n",
    "        \n",
    "        # 1. Temperature-scaled probabilities (main calibration)\n",
    "        calibrated_probs = F.softmax(logits / temperature_param, dim=-1)\n",
    "        \n",
    "        # 2. Calibrated entropy\n",
    "        calibrated_entropy = -(calibrated_probs * torch.log(calibrated_probs + 1e-8)).sum().item()\n",
    "        calibrated_entropies.append(calibrated_entropy)\n",
    "        \n",
    "        # 3. Calibrated confidence\n",
    "        calibrated_confidence = torch.max(calibrated_probs).item()\n",
    "        calibrated_confidences.append(calibrated_confidence)\n",
    "        \n",
    "        # 4. Token disagreement (variance in predictions)\n",
    "        if i > 0:  # Compare with previous token\n",
    "            prev_logits = output.scores[i-1][0]\n",
    "            prev_calibrated_probs = F.softmax(prev_logits / temperature_param, dim=-1)\n",
    "            \n",
    "            # KL divergence between consecutive tokens\n",
    "            kl_div = F.kl_div(\n",
    "                torch.log(calibrated_probs + 1e-8), \n",
    "                prev_calibrated_probs, \n",
    "                reduction='sum'\n",
    "            ).item()\n",
    "            token_disagreements.append(kl_div)\n",
    "        \n",
    "        # 5. Probability mass concentration (how spread out is the distribution)\n",
    "        sorted_probs, _ = torch.sort(calibrated_probs, descending=True)\n",
    "        top_10_mass = torch.sum(sorted_probs[:10]).item()\n",
    "        prob_concentrations.append(top_10_mass)\n",
    "        \n",
    "        # 6. Semantic uncertainty (position-aware)\n",
    "        # Higher uncertainty at beginning/end of sequences\n",
    "        position_weight = 1.0 + 0.5 * abs(i - len(output.scores)/2) / len(output.scores)\n",
    "        semantic_uncertainty = calibrated_entropy * position_weight\n",
    "        semantic_uncertainties.append(semantic_uncertainty)\n",
    "    \n",
    "    # Aggregate metrics\n",
    "    metrics = {\n",
    "        # Calibrated basic metrics\n",
    "        'calibrated_mean_entropy': np.mean(calibrated_entropies),\n",
    "        'calibrated_max_entropy': np.max(calibrated_entropies),\n",
    "        'calibrated_std_entropy': np.std(calibrated_entropies),\n",
    "        'calibrated_mean_confidence': np.mean(calibrated_confidences),\n",
    "        'calibrated_min_confidence': np.min(calibrated_confidences),\n",
    "        \n",
    "        # Advanced calibrated metrics\n",
    "        'token_disagreement_mean': np.mean(token_disagreements) if token_disagreements else 0.0,\n",
    "        'token_disagreement_max': np.max(token_disagreements) if token_disagreements else 0.0,\n",
    "        'prob_concentration_mean': np.mean(prob_concentrations),\n",
    "        'prob_concentration_std': np.std(prob_concentrations),\n",
    "        \n",
    "        # Semantic uncertainty\n",
    "        'semantic_uncertainty_mean': np.mean(semantic_uncertainties),\n",
    "        'semantic_uncertainty_max': np.max(semantic_uncertainties),\n",
    "        \n",
    "        # Distribution shape metrics\n",
    "        'entropy_coefficient_variation': np.std(calibrated_entropies) / (np.mean(calibrated_entropies) + 1e-8),\n",
    "        'confidence_range': np.max(calibrated_confidences) - np.min(calibrated_confidences),\n",
    "        \n",
    "        # Temperature used for calibration\n",
    "        'calibration_temperature': temperature_param\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def calculate_mutual_information_uncertainty(output, temperature_param=1.5):\n",
    "    \"\"\"\n",
    "    Calculate mutual information between consecutive tokens as uncertainty measure.\n",
    "    High MI = tokens are predictable from each other = low uncertainty\n",
    "    \"\"\"\n",
    "    if not hasattr(output, 'scores') or len(output.scores) < 2:\n",
    "        return {'mutual_information': 0.0, 'avg_token_surprise': 0.0}\n",
    "    \n",
    "    mi_scores = []\n",
    "    surprises = []\n",
    "    \n",
    "    for i in range(1, len(output.scores)):\n",
    "        curr_logits = output.scores[i][0]\n",
    "        prev_logits = output.scores[i-1][0]\n",
    "        \n",
    "        # Calibrated probabilities\n",
    "        curr_probs = F.softmax(curr_logits / temperature_param, dim=-1)\n",
    "        prev_probs = F.softmax(prev_logits / temperature_param, dim=-1)\n",
    "        \n",
    "        # Mutual information approximation\n",
    "        joint_entropy = -(curr_probs * torch.log(curr_probs + 1e-8)).sum().item()\n",
    "        conditional_entropy = F.kl_div(\n",
    "            torch.log(curr_probs + 1e-8), \n",
    "            prev_probs, \n",
    "            reduction='sum'\n",
    "        ).item()\n",
    "        \n",
    "        mi = max(0, joint_entropy - conditional_entropy)\n",
    "        mi_scores.append(mi)\n",
    "        \n",
    "        # Token surprise (how unexpected was this token)\n",
    "        actual_token_idx = torch.argmax(curr_logits).item()\n",
    "        surprise = -torch.log(curr_probs[actual_token_idx] + 1e-8).item()\n",
    "        surprises.append(surprise)\n",
    "    \n",
    "    return {\n",
    "        'mutual_information': np.mean(mi_scores),\n",
    "        'avg_token_surprise': np.mean(surprises),\n",
    "        'max_token_surprise': np.max(surprises) if surprises else 0.0\n",
    "    }\n",
    "\n",
    "def get_empty_calibrated_metrics():\n",
    "    \"\"\"Return empty calibrated metrics for error cases\"\"\"\n",
    "    return {\n",
    "        'calibrated_mean_entropy': 0.0,\n",
    "        'calibrated_max_entropy': 0.0,\n",
    "        'calibrated_std_entropy': 0.0,\n",
    "        'calibrated_mean_confidence': 0.0,\n",
    "        'calibrated_min_confidence': 0.0,\n",
    "        'token_disagreement_mean': 0.0,\n",
    "        'token_disagreement_max': 0.0,\n",
    "        'prob_concentration_mean': 0.0,\n",
    "        'prob_concentration_std': 0.0,\n",
    "        'semantic_uncertainty_mean': 0.0,\n",
    "        'semantic_uncertainty_max': 0.0,\n",
    "        'entropy_coefficient_variation': 0.0,\n",
    "        'confidence_range': 0.0,\n",
    "        'calibration_temperature': 1.5,\n",
    "        'mutual_information': 0.0,\n",
    "        'avg_token_surprise': 0.0,\n",
    "        'max_token_surprise': 0.0\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# ORIGINAL UNCERTAINTY METRICS FUNCTIONS (FIXED)\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_entropy_metrics(output):\n",
    "    \"\"\"Calculate entropy-based uncertainty metrics - FIXED\"\"\"\n",
    "    if not hasattr(output, 'scores') or not output.scores:\n",
    "        return {'mean_entropy': 0.0, 'max_entropy': 0.0, 'min_entropy': 0.0, 'std_entropy': 0.0}\n",
    "    \n",
    "    entropies = []\n",
    "    for score in output.scores:\n",
    "        # score is a tensor of shape (batch_size, vocab_size)\n",
    "        # We want the first batch element\n",
    "        logits = score[0]  # Shape: (vocab_size,)\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Calculate entropy: -sum(p * log(p))\n",
    "        # Add epsilon to avoid log(0)\n",
    "        entropy = -(probs * torch.log(probs + 1e-8)).sum().item()\n",
    "        entropies.append(entropy)\n",
    "    \n",
    "    if not entropies:\n",
    "        return {'mean_entropy': 0.0, 'max_entropy': 0.0, 'min_entropy': 0.0, 'std_entropy': 0.0}\n",
    "    \n",
    "    return {\n",
    "        'mean_entropy': np.mean(entropies),\n",
    "        'max_entropy': np.max(entropies),\n",
    "        'min_entropy': np.min(entropies),\n",
    "        'std_entropy': np.std(entropies)\n",
    "    }\n",
    "\n",
    "def calculate_confidence_metrics(output):\n",
    "    \"\"\"Calculate confidence-based metrics - FIXED\"\"\"\n",
    "    if not hasattr(output, 'scores') or not output.scores:\n",
    "        return {'mean_confidence': 0.0, 'min_confidence': 0.0, 'std_confidence': 0.0}\n",
    "    \n",
    "    confidences = []\n",
    "    for score in output.scores:\n",
    "        logits = score[0]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        max_prob = torch.max(probs).item()\n",
    "        confidences.append(max_prob)\n",
    "    \n",
    "    if not confidences:\n",
    "        return {'mean_confidence': 0.0, 'min_confidence': 0.0, 'std_confidence': 0.0}\n",
    "    \n",
    "    return {\n",
    "        'mean_confidence': np.mean(confidences),\n",
    "        'min_confidence': np.min(confidences),\n",
    "        'std_confidence': np.std(confidences)\n",
    "    }\n",
    "\n",
    "def calculate_concentration_metrics(output):\n",
    "    \"\"\"Calculate top-k concentration metrics - FIXED\"\"\"\n",
    "    if not hasattr(output, 'scores') or not output.scores:\n",
    "        return {'mean_top5_conc': 0.0, 'mean_top10_conc': 0.0, 'min_top5_conc': 0.0}\n",
    "    \n",
    "    top5_concs = []\n",
    "    top10_concs = []\n",
    "    \n",
    "    for score in output.scores:\n",
    "        logits = score[0]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        sorted_probs, _ = torch.sort(probs, descending=True)\n",
    "        \n",
    "        top5_conc = torch.sum(sorted_probs[:5]).item()\n",
    "        top10_conc = torch.sum(sorted_probs[:10]).item()\n",
    "        \n",
    "        top5_concs.append(top5_conc)\n",
    "        top10_concs.append(top10_conc)\n",
    "    \n",
    "    if not top5_concs:\n",
    "        return {'mean_top5_conc': 0.0, 'mean_top10_conc': 0.0, 'min_top5_conc': 0.0}\n",
    "    \n",
    "    return {\n",
    "        'mean_top5_conc': np.mean(top5_concs),\n",
    "        'mean_top10_conc': np.mean(top10_concs),\n",
    "        'min_top5_conc': np.min(top5_concs)\n",
    "    }\n",
    "\n",
    "def calculate_sequence_metrics(output):\n",
    "    \"\"\"Calculate sequence-level metrics - FIXED\"\"\"\n",
    "    if not hasattr(output, 'scores') or not output.scores or len(output.scores) < 2:\n",
    "        return {'sequence_length': 0, 'entropy_trend': 0.0, 'confidence_trend': 0.0}\n",
    "    \n",
    "    # Get first and last token metrics for trends\n",
    "    first_logits = output.scores[0][0]\n",
    "    last_logits = output.scores[-1][0]\n",
    "    \n",
    "    first_probs = F.softmax(first_logits, dim=-1)\n",
    "    last_probs = F.softmax(last_logits, dim=-1)\n",
    "    \n",
    "    first_entropy = -(first_probs * torch.log(first_probs + 1e-8)).sum().item()\n",
    "    last_entropy = -(last_probs * torch.log(last_probs + 1e-8)).sum().item()\n",
    "    \n",
    "    first_confidence = torch.max(first_probs).item()\n",
    "    last_confidence = torch.max(last_probs).item()\n",
    "    \n",
    "    return {\n",
    "        'sequence_length': len(output.scores),\n",
    "        'entropy_trend': last_entropy - first_entropy,\n",
    "        'confidence_trend': last_confidence - first_confidence\n",
    "    }\n",
    "\n",
    "def calculate_distribution_metrics(output):\n",
    "    \"\"\"Calculate percentile-based distribution metrics - FIXED\"\"\"\n",
    "    if not hasattr(output, 'scores') or not output.scores:\n",
    "        return {'entropy_p25': 0.0, 'entropy_p75': 0.0, 'confidence_p25': 0.0, 'confidence_p75': 0.0}\n",
    "    \n",
    "    entropies = []\n",
    "    confidences = []\n",
    "    \n",
    "    for score in output.scores:\n",
    "        logits = score[0]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        entropy = -(probs * torch.log(probs + 1e-8)).sum().item()\n",
    "        confidence = torch.max(probs).item()\n",
    "        \n",
    "        entropies.append(entropy)\n",
    "        confidences.append(confidence)\n",
    "    \n",
    "    if not entropies:\n",
    "        return {'entropy_p25': 0.0, 'entropy_p75': 0.0, 'confidence_p25': 0.0, 'confidence_p75': 0.0}\n",
    "    \n",
    "    return {\n",
    "        'entropy_p25': np.percentile(entropies, 25),\n",
    "        'entropy_p75': np.percentile(entropies, 75),\n",
    "        'confidence_p25': np.percentile(confidences, 25),\n",
    "        'confidence_p75': np.percentile(confidences, 75)\n",
    "    }\n",
    "\n",
    "def extract_all_metrics(output, use_calibrated=True, temperature_param=1.5):\n",
    "    \"\"\"Combine original + calibrated uncertainty metrics\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Original metrics (keep these for comparison)\n",
    "    metrics.update(calculate_entropy_metrics(output))\n",
    "    metrics.update(calculate_confidence_metrics(output))\n",
    "    metrics.update(calculate_concentration_metrics(output))\n",
    "    metrics.update(calculate_sequence_metrics(output))\n",
    "    metrics.update(calculate_distribution_metrics(output))\n",
    "    \n",
    "    # NEW: Add calibrated metrics\n",
    "    if use_calibrated:\n",
    "        metrics.update(calculate_calibrated_uncertainty_metrics(output, temperature_param))\n",
    "        metrics.update(calculate_mutual_information_uncertainty(output, temperature_param))\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# =============================================================================\n",
    "# IMPROVED CODE GENERATION PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def generate_single_response(model, tokenizer, instruction, seq_id):\n",
    "    \"\"\"Generate code for single problem and extract metrics - IMPROVED\"\"\"\n",
    "    try:\n",
    "        # Create and format prompt\n",
    "        messages = create_qwen_prompt(instruction)\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # Tokenize and move to device\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        device = next(model.parameters()).device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # IMPROVED GENERATION PARAMETERS FOR UNCERTAINTY EXTRACTION\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "                do_sample=True,  # This is crucial for uncertainty\n",
    "                temperature=1.2,  # HIGHER temperature for more uncertainty spread\n",
    "                top_p=0.85,      # Slightly lower top_p\n",
    "                top_k=40,        # Smaller top_k for more focused sampling\n",
    "                repetition_penalty=1.05,  # Prevent repetitive patterns\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                num_beams=1,     # Single beam for uncertainty\n",
    "                early_stopping=False,  # Let it generate more naturally\n",
    "            )\n",
    "        \n",
    "        # Debug: Check if we have scores\n",
    "        if not hasattr(output, 'scores') or not output.scores:\n",
    "            print(f\"WARNING: No scores returned for {seq_id}\")\n",
    "            return create_empty_result(seq_id, instruction, \"No scores returned\")\n",
    "        \n",
    "        print(f\"DEBUG: Generated {len(output.scores)} tokens with scores for {seq_id}\")\n",
    "        \n",
    "        # Extract generated text and code\n",
    "        generated_text = tokenizer.decode(\n",
    "            output.sequences[0][inputs['input_ids'].shape[1]:], \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        generated_code = extract_code_from_response(generated_text)\n",
    "        \n",
    "        # Extract all uncertainty metrics (including calibrated ones)\n",
    "        metrics = extract_all_metrics(output, use_calibrated=True, temperature_param=1.5)\n",
    "        \n",
    "        # Debug: Print some metrics including calibrated ones\n",
    "        print(f\"  Original  - Entropy: {metrics.get('mean_entropy', 0):.4f}, \"\n",
    "              f\"Confidence: {metrics.get('mean_confidence', 0):.4f}\")\n",
    "        print(f\"  Calibrated- Entropy: {metrics.get('calibrated_mean_entropy', 0):.4f}, \"\n",
    "              f\"Confidence: {metrics.get('calibrated_mean_confidence', 0):.4f}\")\n",
    "        print(f\"  Advanced  - Token Disagreement: {metrics.get('token_disagreement_mean', 0):.4f}, \"\n",
    "              f\"Mutual Info: {metrics.get('mutual_information', 0):.4f}\")\n",
    "        \n",
    "        # Create result record\n",
    "        result = {\n",
    "            'seq_id': seq_id,\n",
    "            'instruction': instruction,\n",
    "            'generated_text': generated_text,\n",
    "            'generated_code': generated_code,\n",
    "            'prompt': prompt\n",
    "        }\n",
    "        \n",
    "        # Add all metrics\n",
    "        result.update(metrics)\n",
    "        \n",
    "        # Clear memory\n",
    "        del output, inputs\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating for {seq_id}: {e}\")\n",
    "        return create_empty_result(seq_id, instruction, str(e))\n",
    "\n",
    "def create_empty_result(seq_id, instruction, error_msg):\n",
    "    \"\"\"Create empty result with zero metrics\"\"\"\n",
    "    result = {\n",
    "        'seq_id': seq_id,\n",
    "        'instruction': instruction,\n",
    "        'generated_text': '',\n",
    "        'generated_code': '',\n",
    "        'prompt': '',\n",
    "        'error': error_msg\n",
    "    }\n",
    "    \n",
    "    # Add zero metrics - original ones\n",
    "    original_metric_names = ['mean_entropy', 'max_entropy', 'min_entropy', 'std_entropy',\n",
    "                           'mean_confidence', 'min_confidence', 'std_confidence',\n",
    "                           'mean_top5_conc', 'mean_top10_conc', 'min_top5_conc',\n",
    "                           'sequence_length', 'entropy_trend', 'confidence_trend',\n",
    "                           'entropy_p25', 'entropy_p75', 'confidence_p25', 'confidence_p75']\n",
    "    \n",
    "    # Add zero metrics - calibrated ones\n",
    "    calibrated_metric_names = ['calibrated_mean_entropy', 'calibrated_max_entropy', 'calibrated_std_entropy',\n",
    "                              'calibrated_mean_confidence', 'calibrated_min_confidence',\n",
    "                              'token_disagreement_mean', 'token_disagreement_max',\n",
    "                              'prob_concentration_mean', 'prob_concentration_std',\n",
    "                              'semantic_uncertainty_mean', 'semantic_uncertainty_max',\n",
    "                              'entropy_coefficient_variation', 'confidence_range',\n",
    "                              'calibration_temperature', 'mutual_information',\n",
    "                              'avg_token_surprise', 'max_token_surprise']\n",
    "    \n",
    "    all_metric_names = original_metric_names + calibrated_metric_names\n",
    "    \n",
    "    for metric in all_metric_names:\n",
    "        result[metric] = 0.0\n",
    "        \n",
    "    return result\n",
    "\n",
    "def create_qwen_prompt(instruction):\n",
    "    \"\"\"Create structured prompt for Qwen model\"\"\"\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a skilled Python programmer. Write clean, working code without comments or explanations. Focus only on solving the problem correctly.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": f\"Write a Python function to solve this problem. Provide only the function code without comments or explanations:\\n\\n{instruction}\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "def extract_code_from_response(response):\n",
    "    \"\"\"Extract Python code from model response\"\"\"\n",
    "    # Look for code blocks first\n",
    "    if \"```python\" in response:\n",
    "        start = response.find(\"```python\") + 9\n",
    "        end = response.find(\"```\", start)\n",
    "        if end != -1:\n",
    "            return response[start:end].strip()\n",
    "    \n",
    "    if \"```\" in response:\n",
    "        start = response.find(\"```\") + 3\n",
    "        end = response.find(\"```\", start)\n",
    "        if end != -1:\n",
    "            return response[start:end].strip()\n",
    "    \n",
    "    # Look for function definitions\n",
    "    lines = response.split('\\n')\n",
    "    code_lines = []\n",
    "    in_function = False\n",
    "    \n",
    "    for line in lines:\n",
    "        if line.strip().startswith('def '):\n",
    "            in_function = True\n",
    "        if in_function:\n",
    "            code_lines.append(line)\n",
    "    \n",
    "    return '\\n'.join(code_lines).strip() if code_lines else response.strip()\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN GENERATION FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def generate_code_with_metrics(csv_path, model, tokenizer, output_dir=\"generation_results\", save_every=50, max_samples=None):\n",
    "    \"\"\"\n",
    "    Generate code for all problems and extract metrics.\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to input CSV\n",
    "        model: Your language model\n",
    "        tokenizer: Your tokenizer\n",
    "        output_dir: Directory to save results\n",
    "        save_every: Save progress every N generations\n",
    "        max_samples: Limit number of samples (None for all)\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load dataset\n",
    "    print(\"Loading dataset...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    if max_samples:\n",
    "        df = df.head(max_samples)\n",
    "        print(f\"Limited to {len(df)} samples\")\n",
    "    else:\n",
    "        print(f\"Loaded {len(df)} problems\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        print(f\"\\nGenerating {idx+1}/{len(df)}: {row['seq_id']}\")\n",
    "        \n",
    "        # Generate single response with metrics\n",
    "        result = generate_single_response(\n",
    "            model, tokenizer, row['instruction'], row['seq_id']\n",
    "        )\n",
    "        \n",
    "        # Add original data\n",
    "        result.update({\n",
    "            'original_output': row['output'],\n",
    "            'original_code': row['code'],\n",
    "            'entry_point': row['entry_point'],\n",
    "            'testcase': row['testcase']\n",
    "        })\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "        # Print key metrics\n",
    "        if 'error' not in result:\n",
    "            print(f\"  Final Summary - Original Entropy: {result['mean_entropy']:.4f}, \"\n",
    "                  f\"Calibrated Entropy: {result['calibrated_mean_entropy']:.4f}\")\n",
    "            print(f\"                  Original Confidence: {result['mean_confidence']:.4f}, \"\n",
    "                  f\"Calibrated Confidence: {result['calibrated_mean_confidence']:.4f}\")\n",
    "        else:\n",
    "            print(f\"  Error: {result['error']}\")\n",
    "        \n",
    "        # Save progress every N generations\n",
    "        if (idx + 1) % save_every == 0 or idx == len(df) - 1:\n",
    "            df_batch = pd.DataFrame(results)\n",
    "            save_path = os.path.join(output_dir, f\"generations_up_to_{idx+1}.csv\")\n",
    "            df_batch.to_csv(save_path, index=False)\n",
    "            print(f\"\\nSaved progress to {save_path}\")\n",
    "    \n",
    "    # Final save\n",
    "    final_df = pd.DataFrame(results)\n",
    "    final_path = os.path.join(output_dir, \"all_generations_with_calibrated_metrics.csv\")\n",
    "    final_df.to_csv(final_path, index=False)\n",
    "    \n",
    "    print(f\"\\n✅ Generation complete! Saved {len(results)} results to {final_path}\")\n",
    "    \n",
    "    # Enhanced summary with calibrated metrics\n",
    "    successful = len([r for r in results if 'error' not in r and r['mean_entropy'] > 0])\n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"- Successful generations: {successful}/{len(results)}\")\n",
    "    \n",
    "    if successful > 0:\n",
    "        valid_results = [r for r in results if 'error' not in r and r['mean_entropy'] > 0]\n",
    "        \n",
    "        # Original metrics summary\n",
    "        print(f\"\\nOriginal Metrics:\")\n",
    "        print(f\"- Average entropy: {np.mean([r['mean_entropy'] for r in valid_results]):.4f}\")\n",
    "        print(f\"- Average confidence: {np.mean([r['mean_confidence'] for r in valid_results]):.4f}\")\n",
    "        print(f\"- Entropy range: {np.min([r['mean_entropy'] for r in valid_results]):.4f} - {np.max([r['mean_entropy'] for r in valid_results]):.4f}\")\n",
    "        \n",
    "        # Calibrated metrics summary\n",
    "        print(f\"\\nCalibrated Metrics:\")\n",
    "        print(f\"- Average calibrated entropy: {np.mean([r['calibrated_mean_entropy'] for r in valid_results]):.4f}\")\n",
    "        print(f\"- Average calibrated confidence: {np.mean([r['calibrated_mean_confidence'] for r in valid_results]):.4f}\")\n",
    "        print(f\"- Average token disagreement: {np.mean([r['token_disagreement_mean'] for r in valid_results]):.4f}\")\n",
    "        print(f\"- Average mutual information: {np.mean([r['mutual_information'] for r in valid_results]):.4f}\")\n",
    "        print(f\"- Calibrated entropy range: {np.min([r['calibrated_mean_entropy'] for r in valid_results]):.4f} - {np.max([r['calibrated_mean_entropy'] for r in valid_results]):.4f}\")\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import torch\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "    \n",
    "    # Setup quantized model\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "    )\n",
    "    \n",
    "    model_name = \"unsloth/Qwen2.5-Coder-7B-Instruct-bnb-4bit\"\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Ensure tokenizer has pad token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(\"Model loaded successfully!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Phase 1: Generate code and extract metrics (including calibrated uncertainty)\n",
    "    print(\"STARTING PHASE 1: Code generation with calibrated uncertainty metrics\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    generations_df = generate_code_with_metrics(\n",
    "        csv_path=\"C:/Users/s/Desktop/Dev/SamsungProject/extract/top-30-educational-instruct-rows.csv\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        output_dir=\"generation_results_calibrated\",\n",
    "        save_every=25,  # Save every 25 generations\n",
    "        max_samples=50  # Start with 50 samples to test\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PHASE 1 COMPLETE: Code generation with calibrated uncertainty metrics\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Generated files:\")\n",
    "    print(\"- all_generations_with_calibrated_metrics.csv\")\n",
    "    print(\"\\nKey improvements:\")\n",
    "    print(\"✅ Temperature calibration (1.2 generation, 1.5 post-processing)\")\n",
    "    print(\"✅ Token disagreement metrics\")\n",
    "    print(\"✅ Mutual information uncertainty\")\n",
    "    print(\"✅ Semantic uncertainty (position-aware)\")\n",
    "    print(\"✅ Better sampling parameters\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"1. Inspect the calibrated metrics - should see more variation!\")\n",
    "    print(\"2. Compare original vs calibrated uncertainty distributions\")\n",
    "    print(\"3. Run testing phase to create labels using Pass@1\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25fbb2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samsungProj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
