{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9ef4e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s\\miniconda3\\envs\\samsungProj\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "import json\n",
    "import pandas as pd\n",
    "import ast\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bb43271",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s\\miniconda3\\envs\\samsungProj\\Lib\\site-packages\\transformers\\quantizers\\auto.py:226: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"unsloth/Qwen2.5-Coder-7B-Instruct-bnb-4bit\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cc200d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0.dev20250801+cu129\n",
      "CUDA available: True\n",
      "âœ… Model loaded successfully!\n",
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(\"âœ… Model loaded successfully!\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a9b7ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token lengths for 'code':\n",
      "           seq_id              entry_point  code_token_length\n",
      "3250  38901396701  next_closest_palindrome                681\n",
      "8674  30074294089          transform_words                504\n",
      "8373  14120506416             ideal_arrays                496\n",
      "5809   1185920464     is_perfect_rectangle                487\n",
      "2788   7804968646          is_valid_number                479\n",
      "...           ...                      ...                ...\n",
      "6914   1516364756          smallest_number                 10\n",
      "8847  31357937629                 list_sum                 10\n",
      "2173  24802847166                 find_max                 10\n",
      "8978  14699557128          smallest_number                 10\n",
      "3932  41065408432          smallest_number                 10\n",
      "\n",
      "[10000 rows x 3 columns]\n",
      "\n",
      "ðŸŽ¯ Maximum token length in 'code': 681\n",
      "\n",
      "ðŸ“Š Statistics:\n",
      "count    10000.000000\n",
      "mean        80.683000\n",
      "std         56.665416\n",
      "min         10.000000\n",
      "25%         42.000000\n",
      "50%         67.000000\n",
      "75%        102.000000\n",
      "max        681.000000\n",
      "Name: code_token_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 1. Load the dataset\n",
    "csv_path = \"C:\\\\Users\\\\s\\\\Desktop\\\\Dev\\\\SamsungProject\\\\extract\\\\code_dataset_10k.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# 2. Initialize the tokenizer (use the same one your model uses)\n",
    "#    Since you're using Qwen, we'll use its tokenizer\n",
    "model_name = \"unsloth/Qwen2.5-Coder-14B-Instruct-bnb-4bit\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 3. Function to count tokens\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.encode(str(text)))\n",
    "\n",
    "# 4. Add token count column for the 'code' field\n",
    "df['code_token_length'] = df['code'].apply(count_tokens)\n",
    "\n",
    "# 5. Show results\n",
    "print(\"Token lengths for 'code':\")\n",
    "print(df[['seq_id', 'entry_point', 'code_token_length']].sort_values(by='code_token_length', ascending=False))\n",
    "\n",
    "# 6. Get maximum token length\n",
    "max_tokens = df['code_token_length'].max()\n",
    "print(f\"\\nðŸŽ¯ Maximum token length in 'code': {max_tokens}\")\n",
    "\n",
    "# Optional: Get some statistics\n",
    "print(f\"\\nðŸ“Š Statistics:\")\n",
    "print(df['code_token_length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b439e4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 30 entries by code token length (sorted descending):\n",
      "           seq_id              entry_point  \\\n",
      "0     38901396701  next_closest_palindrome   \n",
      "1     30074294089          transform_words   \n",
      "2     14120506416             ideal_arrays   \n",
      "3      1185920464     is_perfect_rectangle   \n",
      "4      7804968646          is_valid_number   \n",
      "...           ...                      ...   \n",
      "9995   1516364756          smallest_number   \n",
      "9996  31357937629                 list_sum   \n",
      "9997  24802847166                 find_max   \n",
      "9998  14699557128          smallest_number   \n",
      "9999  41065408432          smallest_number   \n",
      "\n",
      "                                                   code  code_token_length  \n",
      "0     from functools import reduce\\n\\ndef next_close...                681  \n",
      "1     from collections import defaultdict\\n\\ndef tra...                504  \n",
      "2     from collections import Counter\\nfrom functool...                496  \n",
      "3     from typing import List\\n\\nclass Solution:\\n  ...                487  \n",
      "4     def is_valid_number(s):\\n    s = s.strip()\\n  ...                479  \n",
      "...                                                 ...                ...  \n",
      "9995     def smallest_number(arr):\\n    return min(arr)                 10  \n",
      "9996          def list_sum(nums):\\n    return sum(nums)                 10  \n",
      "9997          def find_max(nums):\\n    return max(nums)                 10  \n",
      "9998  def smallest_number(numbers):\\n    return min(...                 10  \n",
      "9999   def smallest_number(nums):\\n    return min(nums)                 10  \n",
      "\n",
      "[10000 rows x 4 columns]\n",
      "\n",
      "ðŸŽ¯ Maximum token length in 'code': 681\n",
      "\n",
      "âœ… Saved sorted results to 'top_30_by_code_token_length.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 1. Load the dataset\n",
    "csv_path = \"C:\\\\Users\\\\s\\\\Desktop\\\\Dev\\\\SamsungProject\\\\extract\\\\code_dataset_10k.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# 2. Initialize the Qwen tokenizer\n",
    "model_name = \"unsloth/Qwen2.5-Coder-14B-Instruct-bnb-4bit\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 3. Function to count tokens\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.encode(str(text)))\n",
    "\n",
    "# 4. Add token count column for the 'code' field\n",
    "df['code_token_length'] = df['code'].apply(count_tokens)\n",
    "\n",
    "# 5. Sort by token length in descending order and show top 30\n",
    "top_30_by_tokens = df[['seq_id', 'entry_point', 'code', 'code_token_length']] \\\n",
    "    .sort_values(by='code_token_length', ascending=False) \\\n",
    "    .reset_index(drop=True)\n",
    "\n",
    "# 6. Display results\n",
    "print(\"Top 30 entries by code token length (sorted descending):\")\n",
    "print(top_30_by_tokens)\n",
    "\n",
    "# 7. Show the maximum token count\n",
    "max_tokens = top_30_by_tokens['code_token_length'].max()\n",
    "print(f\"\\nðŸŽ¯ Maximum token length in 'code': {max_tokens}\")\n",
    "\n",
    "# Optional: Save this ranked list to a new CSV\n",
    "top_30_by_tokens.to_csv(\"top_30_by_code_token_length.csv\", index=False)\n",
    "print(f\"\\nâœ… Saved sorted results to 'top_30_by_code_token_length.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaecfbe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samsungProj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
